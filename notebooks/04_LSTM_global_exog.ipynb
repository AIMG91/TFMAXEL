{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e499a089",
   "metadata": {},
   "source": [
    "# 04 — LSTM global con exógenas\n",
    "\n",
    "**Objetivo:** forecasting de `Weekly_Sales` semanal por `Store` usando LSTM global con covariables exógenas.\n",
    "\n",
    "## Supuesto experimental (oracle exog)\n",
    "Se asume disponibilidad de todas las covariables exógenas durante el horizonte de predicción (escenario oracle).\n",
    "\n",
    "## Outputs estándar\n",
    "- `outputs/predictions/lstm_exog_predictions.csv` con: `Store, Date, y_true, y_pred, model`\n",
    "- `outputs/metrics/lstm_exog_metrics_global.csv`\n",
    "- `outputs/metrics/lstm_exog_metrics_by_store.csv`\n",
    "- `outputs/figures/lstm_exog_plot_*.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74d2be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Imports y configuración\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.common import (\n",
    "    compute_metrics,\n",
    "    load_data,\n",
    "    make_features,\n",
    "    save_outputs,\n",
    "    temporal_split,\n",
    ")\n",
    "\n",
    "MODEL_NAME = 'lstm_exog'\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = PROJECT_ROOT / 'data' / 'Walmart_Sales.csv'\n",
    "METADATA_PATH = PROJECT_ROOT / 'outputs' / 'metadata.json'\n",
    "OUTPUTS_DIR = PROJECT_ROOT / 'outputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f376ed20",
   "metadata": {},
   "source": [
    "## 1) Cargar metadata (split + features)\n",
    "Esto garantiza consistencia entre modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba822848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: {'train_start': '2010-02-05', 'train_end': '2012-07-06', 'val_start': '2012-07-13', 'val_end': '2012-08-31', 'test_start': '2012-09-07', 'test_end': '2012-10-26'}\n",
      "N features: 16\n"
     ]
    }
   ],
   "source": [
    "metadata = json.loads(METADATA_PATH.read_text(encoding='utf-8'))\n",
    "split = metadata['split']\n",
    "feature_cols = metadata['features']\n",
    "print('Split:', split)\n",
    "print('N features:', len(feature_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2242b1",
   "metadata": {},
   "source": [
    "## 2) Carga de datos + features\n",
    "- Parseo/orden\n",
    "- Construcción de lags/rolling (sin leakage)\n",
    "- Exógenas alineadas por fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5a599d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4095, 22)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data(DATA_PATH)\n",
    "df_feat, _ = make_features(df, add_calendar=True)\n",
    "\n",
    "# Importante: para entrenar, debes decidir cómo tratar NaNs creados por lags/rolling\n",
    "# Opción típica: descartar filas con NaNs en features (por store al inicio)\n",
    "model_df = df_feat.dropna(subset=feature_cols + ['Weekly_Sales']).copy()\n",
    "model_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b32f0b",
   "metadata": {},
   "source": [
    "## 3) Split temporal\n",
    "Reutiliza exactamente el split definido en el notebook 00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2b840e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3375 360 360\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df, split_cfg = temporal_split(df)\n",
    "\n",
    "# Aplicar el split sobre model_df (ya sin NaNs por lags)\n",
    "train = model_df[model_df['Date'].between(split_cfg.train_start, split_cfg.train_end)].copy()\n",
    "val = model_df[model_df['Date'].between(split_cfg.val_start, split_cfg.val_end)].copy()\n",
    "test = model_df[model_df['Date'].between(split_cfg.test_start, split_cfg.test_end)].copy()\n",
    "\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5faddbe",
   "metadata": {},
   "source": [
    "## 4) Entrenamiento del modelo\n",
    "Implementación LSTM global con covariables exógenas.\n",
    "Incluye representaciones de Store (one-hot o embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4f2c21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implementar entrenamiento LSTM con exógenas\n",
    "# Debe producir predicciones para TEST (ideal: también para VAL).\n",
    "y_pred_test = np.full(shape=len(test), fill_value=test['Weekly_Sales'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55a0b31",
   "metadata": {},
   "source": [
    "## 5) Métricas (MAE, RMSE, sMAPE)\n",
    "Se reporta:\n",
    "- Global\n",
    "- Por store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "956cd887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper para construir features autoregresivas (sin leakage)\n",
    "\n",
    "def _compute_feature_vector(y_hist, date, exog_row, cfg):\n",
    "\n",
    "    lags = cfg[\"lags\"]\n",
    "\n",
    "    rollings = cfg[\"rollings\"]\n",
    "\n",
    "    add_calendar = bool(cfg.get(\"add_calendar\", True))\n",
    "\n",
    "    exog_cols = cfg[\"exog_cols\"]\n",
    "\n",
    "    feat = {}\n",
    "\n",
    "    for k in lags:\n",
    "\n",
    "        feat[f\"lag_{k}\"] = y_hist[-k] if len(y_hist) >= k else np.nan\n",
    "\n",
    "    for w in rollings:\n",
    "\n",
    "        if len(y_hist) >= w:\n",
    "\n",
    "            window = np.array(y_hist[-w:], dtype=float)\n",
    "\n",
    "            feat[f\"roll_mean_{w}\"] = float(window.mean())\n",
    "\n",
    "            feat[f\"roll_std_{w}\"] = float(window.std(ddof=0))\n",
    "\n",
    "        else:\n",
    "\n",
    "            feat[f\"roll_mean_{w}\"] = np.nan\n",
    "\n",
    "            feat[f\"roll_std_{w}\"] = np.nan\n",
    "\n",
    "    for c in exog_cols:\n",
    "\n",
    "        feat[c] = float(exog_row[c])\n",
    "\n",
    "    if add_calendar:\n",
    "\n",
    "        iso = pd.Timestamp(date).isocalendar()\n",
    "\n",
    "        feat[\"weekofyear\"] = int(iso.week)\n",
    "\n",
    "        feat[\"month\"] = int(pd.Timestamp(date).month)\n",
    "\n",
    "        feat[\"year\"] = int(pd.Timestamp(date).year)\n",
    "\n",
    "    vec = [feat.get(c, np.nan) for c in feature_cols]\n",
    "\n",
    "    return vec\n",
    "\n",
    "\n",
    "\n",
    "# ... resto del código previo ...\n",
    "\n",
    "\n",
    "\n",
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    import torch\n",
    "\n",
    "    from torch import nn\n",
    "\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "except Exception as exc:\n",
    "\n",
    "    raise ImportError(\"PyTorch no está instalado. Instala con: pip install torch\") from exc\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "# Configuración base + search space controlado\n",
    "\n",
    "EPOCHS_MAX = 200\n",
    "\n",
    "PATIENCE_ES = 15\n",
    "\n",
    "MIN_DELTA = 1e-4\n",
    "\n",
    "CLIP_NORM = 1.0\n",
    "\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "REDUCE_LR_FACTOR = 0.5\n",
    "\n",
    "REDUCE_LR_PATIENCE = 5\n",
    "\n",
    "EMB_DIM = 16\n",
    "\n",
    "WINDOW = 52\n",
    "\n",
    "\n",
    "\n",
    "# Grilla pequeña (<=8 configs)\n",
    "\n",
    "lstm_search = [\n",
    "\n",
    "    {\"hidden_size\": 64, \"num_layers\": 2, \"dropout\": 0.1, \"lr\": 1e-3},\n",
    "\n",
    "    {\"hidden_size\": 64, \"num_layers\": 2, \"dropout\": 0.2, \"lr\": 3e-4},\n",
    "\n",
    "    {\"hidden_size\": 128, \"num_layers\": 2, \"dropout\": 0.1, \"lr\": 3e-4},\n",
    "\n",
    "    {\"hidden_size\": 128, \"num_layers\": 3, \"dropout\": 0.2, \"lr\": 1e-3},\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Anti-leakage: máscaras por fecha no deben solaparse\n",
    "\n",
    "assert split_cfg.train_end < split_cfg.val_start < split_cfg.val_end < split_cfg.test_start <= split_cfg.test_end\n",
    "\n",
    "\n",
    "\n",
    "# Mapping Store -> idx para embeddings\n",
    "\n",
    "stores_sorted = sorted(model_df[\"Store\"].unique())\n",
    "\n",
    "store_to_idx = {s: i for i, s in enumerate(stores_sorted)}\n",
    "\n",
    "num_stores = len(store_to_idx)\n",
    "\n",
    "\n",
    "\n",
    "feature_cols_model = feature_cols  # lags/rolling/exógenas/calendario\n",
    "\n",
    "DEFAULT_LAGS = sorted({int(c.split(\"_\")[1]) for c in feature_cols if c.startswith(\"lag_\")})\n",
    "\n",
    "DEFAULT_ROLLINGS = sorted({int(c.split(\"_\")[2]) for c in feature_cols if c.startswith(\"roll_mean_\")})\n",
    "\n",
    "EXOG_COLUMNS = [c for c in feature_cols if not c.startswith(\"lag_\") and not c.startswith(\"roll_\")]\n",
    "\n",
    "\n",
    "\n",
    "# Escaladores SOLO con train\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "train_fit = train.copy()\n",
    "\n",
    "scaler_x.fit(train_fit[feature_cols_model].values)\n",
    "\n",
    "scaler_y.fit(train_fit[[\"Weekly_Sales\"]].values)\n",
    "\n",
    "assert hasattr(scaler_x, \"mean_\") and hasattr(scaler_y, \"mean_\")\n",
    "\n",
    "\n",
    "\n",
    "def build_sequences(df_in: pd.DataFrame, window: int = WINDOW):\n",
    "\n",
    "    sequences, targets, dates, stores_idx = [], [], [], []\n",
    "\n",
    "    df_in = df_in.sort_values([\"Store\", \"Date\"]).copy()\n",
    "\n",
    "    for store, g in df_in.groupby(\"Store\"):\n",
    "\n",
    "        g = g.sort_values(\"Date\")\n",
    "\n",
    "        X = scaler_x.transform(g[feature_cols_model].values)\n",
    "\n",
    "        y = scaler_y.transform(g[[\"Weekly_Sales\"]].values).ravel()\n",
    "\n",
    "        date_arr = g[\"Date\"].values\n",
    "\n",
    "        s_idx = store_to_idx[int(store)]\n",
    "\n",
    "        for t in range(window, len(g)):\n",
    "\n",
    "            sequences.append(X[t - window : t])\n",
    "\n",
    "            targets.append(y[t])\n",
    "\n",
    "            dates.append(date_arr[t])\n",
    "\n",
    "            stores_idx.append(s_idx)\n",
    "\n",
    "    return (\n",
    "\n",
    "        np.array(sequences),\n",
    "\n",
    "        np.array(targets),\n",
    "\n",
    "        np.array(dates),\n",
    "\n",
    "        np.array(stores_idx, dtype=int),\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "X_seq, y_seq, d_seq, s_seq = build_sequences(model_df, window=WINDOW)\n",
    "\n",
    "\n",
    "\n",
    "# Split por fecha (sin leakage)\n",
    "\n",
    "train_mask = (d_seq >= split_cfg.train_start) & (d_seq <= split_cfg.train_end)\n",
    "\n",
    "val_mask = (d_seq >= split_cfg.val_start) & (d_seq <= split_cfg.val_end)\n",
    "\n",
    "test_mask = (d_seq >= split_cfg.test_start) & (d_seq <= split_cfg.test_end)\n",
    "\n",
    "assert not (train_mask & val_mask).any() and not (train_mask & test_mask).any() and not (val_mask & test_mask).any()\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train, s_train = X_seq[train_mask], y_seq[train_mask], s_seq[train_mask]\n",
    "\n",
    "X_val, y_val, s_val = X_seq[val_mask], y_seq[val_mask], s_seq[val_mask]\n",
    "\n",
    "X_test_seq, y_test_seq, s_test_seq = X_seq[test_mask], y_seq[test_mask], s_seq[test_mask]\n",
    "\n",
    "test_dates = d_seq[test_mask]\n",
    "\n",
    "test_stores = s_seq[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "\n",
    "    TensorDataset(\n",
    "\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "\n",
    "        torch.tensor(s_train, dtype=torch.long),\n",
    "\n",
    "        torch.tensor(y_train, dtype=torch.float32),\n",
    "\n",
    "    ),\n",
    "\n",
    "    batch_size=64,\n",
    "\n",
    "    shuffle=True,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "val_loader = DataLoader(\n",
    "\n",
    "    TensorDataset(\n",
    "\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "\n",
    "        torch.tensor(s_val, dtype=torch.long),\n",
    "\n",
    "        torch.tensor(y_val, dtype=torch.float32),\n",
    "\n",
    "    ),\n",
    "\n",
    "    batch_size=64,\n",
    "\n",
    "    shuffle=False,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float, emb_dim: int, num_stores: int):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.store_emb = nn.Embedding(num_stores, emb_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size + emb_dim, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, store_idx):\n",
    "\n",
    "        emb = self.store_emb(store_idx)\n",
    "\n",
    "        emb_expanded = emb.unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "\n",
    "        x_cat = torch.cat([x, emb_expanded], dim=-1)\n",
    "\n",
    "        out, _ = self.lstm(x_cat)\n",
    "\n",
    "        last = out[:, -1, :]\n",
    "\n",
    "        return self.fc(last).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "def train_eval(config_run: dict):\n",
    "\n",
    "    model = LSTMRegressor(\n",
    "\n",
    "        input_size=X_train.shape[-1],\n",
    "\n",
    "        hidden_size=config_run[\"hidden_size\"],\n",
    "\n",
    "        num_layers=config_run[\"num_layers\"],\n",
    "\n",
    "        dropout=config_run[\"dropout\"],\n",
    "\n",
    "        emb_dim=EMB_DIM,\n",
    "\n",
    "        num_stores=num_stores,\n",
    "\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa5b457",
   "metadata": {},
   "source": [
    "## 5) Métricas (MAE, RMSE, sMAPE)\n",
    "Se reporta:\n",
    "- Global\n",
    "- Por store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15f92514",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       model            MAE           RMSE      sMAPE\n",
       " 0  lstm_exog  441124.924609  518987.216845  46.104407,\n",
       "        model  Store           MAE          RMSE       sMAPE\n",
       " 0  lstm_exog      1  5.320871e+05  5.377143e+05   41.427339\n",
       " 1  lstm_exog      2  8.449641e+05  8.478403e+05   58.743041\n",
       " 2  lstm_exog      3  6.008675e+05  6.010464e+05   84.246824\n",
       " 3  lstm_exog      4  1.108559e+06  1.109581e+06   70.660465\n",
       " 4  lstm_exog      5  6.927589e+05  6.929573e+05  103.826028)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame({\n",
    "    'Store': test['Store'].astype(int).values,\n",
    "    'Date': test['Date'].values,\n",
    "    'y_true': test['Weekly_Sales'].values,\n",
    "    'y_pred': np.asarray(y_pred_test, dtype=float),\n",
    "    'model': MODEL_NAME,\n",
    "})\n",
    "\n",
    "global_metrics = compute_metrics(pred_df['y_true'].values, pred_df['y_pred'].values)\n",
    "metrics_global_df = pd.DataFrame([{'model': MODEL_NAME, **global_metrics}])\n",
    "\n",
    "by_store = []\n",
    "for store, g in pred_df.groupby('Store'):\n",
    "    m = compute_metrics(g['y_true'].values, g['y_pred'].values)\n",
    "    by_store.append({'model': MODEL_NAME, 'Store': int(store), **m})\n",
    "metrics_by_store_df = pd.DataFrame(by_store).sort_values('Store')\n",
    "\n",
    "metrics_global_df, metrics_by_store_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39e50d",
   "metadata": {},
   "source": [
    "## 6) Guardado de outputs estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d9ec213",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': 'c:\\\\Users\\\\usuario\\\\Documents\\\\Master AI\\\\TFM\\\\MEMORIA 2.0\\\\outputs\\\\predictions\\\\lstm_exog_predictions.csv',\n",
       " 'metrics_global': 'c:\\\\Users\\\\usuario\\\\Documents\\\\Master AI\\\\TFM\\\\MEMORIA 2.0\\\\outputs\\\\metrics\\\\lstm_exog_metrics_global.csv',\n",
       " 'metrics_by_store': 'c:\\\\Users\\\\usuario\\\\Documents\\\\Master AI\\\\TFM\\\\MEMORIA 2.0\\\\outputs\\\\metrics\\\\lstm_exog_metrics_by_store.csv'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = save_outputs(\n",
    "    model_name=MODEL_NAME,\n",
    "    predictions=pred_df,\n",
    "    metrics_global=metrics_global_df,\n",
    "    metrics_by_store=metrics_by_store_df,\n",
    "    output_dir=OUTPUTS_DIR,\n",
    ")\n",
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b2d7eb",
   "metadata": {},
   "source": [
    "## 7) Figuras\n",
    "- 3 tiendas: real vs predicción en test\n",
    "- Distribución del error (`y_true - y_pred`)\n",
    "\n",
    "Guardar PNGs en `outputs/figures/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65372443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "FIG_DIR = OUTPUTS_DIR / \"figures\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Selección de 3 tiendas (mayor media de ventas en test)\n",
    "top_stores = (\n",
    "    pred_df.groupby(\"Store\")[\"y_true\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(3)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "for store in top_stores:\n",
    "    g = pred_df[pred_df[\"Store\"] == store].sort_values(\"Date\")\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(g[\"Date\"], g[\"y_true\"], label=\"y_true\")\n",
    "    plt.plot(g[\"Date\"], g[\"y_pred\"], label=\"y_pred\")\n",
    "    plt.title(f\"Store {store} — LSTM\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Weekly_Sales\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{MODEL_NAME}_plot_store_{store}.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# Distribución de error\n",
    "errors = pred_df[\"y_true\"] - pred_df[\"y_pred\"]\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(errors, bins=30, kde=True)\n",
    "plt.title(\"Error distribution (y_true - y_pred)\")\n",
    "plt.xlabel(\"Error\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / f\"{MODEL_NAME}_plot_error_dist.png\", dpi=150)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b1dbdf83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>sMAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lstm_exog</td>\n",
       "      <td>441124.924609</td>\n",
       "      <td>518987.216845</td>\n",
       "      <td>46.104407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model            MAE           RMSE      sMAPE\n",
       "0  lstm_exog  441124.924609  518987.216845  46.104407"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reconstruir results_df si no está en memoria\n",
    "import pandas as pd\n",
    "\n",
    "if \"results_df\" not in globals():\n",
    "    if \"results\" in globals() and len(results) > 0:\n",
    "        rows = []\n",
    "        for r in results:\n",
    "            row = {k: v for k, v in r.items() if k not in {\"state_dict\", \"model_state\"}}\n",
    "            cfg = row.pop(\"config\", {})\n",
    "            if isinstance(cfg, dict):\n",
    "                row.update(cfg)\n",
    "            rows.append(row)\n",
    "        results_df = pd.DataFrame(rows)\n",
    "    elif \"metrics_global_df\" in globals():\n",
    "        # Fallback: usar métricas globales si no hubo búsqueda\n",
    "        results_df = metrics_global_df.copy()\n",
    "    else:\n",
    "        raise ValueError(\"No hay datos para construir results_df; vuelve a correr la celda de entrenamiento.\")\n",
    "\n",
    "if not results_df.empty and all(col in results_df.columns for col in [\"sMAPE\", \"MAE\"]):\n",
    "    results_df = results_df.sort_values([\"sMAPE\", \"MAE\"]).reset_index(drop=True)\n",
    "\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "038802e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor config (ordenada por sMAPE):\n",
      "model        lstm_exog\n",
      "MAE      441124.924609\n",
      "RMSE     518987.216845\n",
      "sMAPE        46.104407\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Baseline (EPOCHS=20) encontrado:\n",
      "       model           MAE         RMSE     sMAPE\n",
      "0  lstm_exog  75827.527682  98575.42425  8.016137\n",
      "Comparación sMAPE delta: 38.088270612307355\n"
     ]
    }
   ],
   "source": [
    "# Resumen rápido: mejor config y (si existe) baseline de 20 epochs\n",
    "\n",
    "best_row = results_df.iloc[0]\n",
    "\n",
    "print(\"Mejor config (ordenada por sMAPE):\")\n",
    "\n",
    "print(best_row)\n",
    "\n",
    "\n",
    "\n",
    "import pathlib\n",
    "\n",
    "baseline_path = pathlib.Path(OUTPUTS_DIR) / \"metrics\" / \"lstm_exog_metrics_global.csv\"\n",
    "\n",
    "if baseline_path.exists():\n",
    "\n",
    "    baseline_df = pd.read_csv(baseline_path)\n",
    "\n",
    "    print(\"\\nBaseline (EPOCHS=20) encontrado:\")\n",
    "\n",
    "    print(baseline_df)\n",
    "\n",
    "    print(\"Comparación sMAPE delta:\", float(best_row[\"sMAPE\"]) - float(baseline_df.loc[0, \"sMAPE\"]))\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"\\nBaseline (EPOCHS=20) no encontrado; ejecuta el baseline para comparar.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
