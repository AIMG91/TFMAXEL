{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2fd41d1",
   "metadata": {},
   "source": [
    "# 02 — Ejecutar E0 (ablación FS0/FS1/FS2)\n",
    "\n",
    "Este notebook ejecuta el experimento **E0** para comparar el impacto de diferentes conjuntos de features (FS0/FS1/FS2) sobre modelos deep globales (LSTM / Transformer).\n",
    "\n",
    "## Split\n",
    "Usamos un split temporal global:\n",
    "- `val_weeks = 8`\n",
    "- `test_weeks = 39` (coherente con la evaluación final del proyecto)\n",
    "\n",
    "Estrategia de evaluación:\n",
    "1) Entrenar con **train** y evaluar en **val**.\n",
    "2) Reentrenar con **train+val** y evaluar en **test**.\n",
    "\n",
    "## Anti-leakage\n",
    "- Features causales (lags/rollings) vienen de `src.common.make_features`.\n",
    "- En modelos deep, el escalado se ajusta con el dataframe de entrenamiento del paso correspondiente.\n",
    "\n",
    "**Requisito**: ejecuta antes el notebook 01 para generar `outputs/E0_ablation/feature_sets.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_state": "idle",
   "id": "7e840f73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T00:00:20.946265Z",
     "iopub.status.busy": "2026-02-01T00:00:20.945956Z",
     "iopub.status.idle": "2026-02-01T00:00:23.121086Z",
     "shell.execute_reply": "2026-02-01T00:00:23.120287Z",
     "shell.execute_reply.started": "2026-02-01T00:00:20.946239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: /mnt/custom-file-systems/s3/shared/TFMAXEL\n",
      "DATA_PATH: /mnt/custom-file-systems/s3/shared/TFMAXEL/data/Walmart_Sales.csv\n",
      "OUTPUT_DIR: /mnt/custom-file-systems/s3/shared/TFMAXEL/outputs/E0_ablation\n",
      "device: cuda {'torch_cuda_available': True, 'cuda_device_name': 'Tesla T4', 'cuda_device_count': 1, 'cuda_capability': (7, 5)}\n",
      "seed: {'seed': 42, 'deterministic': False, 'numpy': 'ok', 'torch': 'ok'}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "import time\n",
    "\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Ensure PROJECT_ROOT is on sys.path so `import src.*` works reliably\n",
    "\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "\n",
    "PROJECT_ROOT = NOTEBOOK_DIR\n",
    "\n",
    "if (PROJECT_ROOT / 'src').exists() is False and (PROJECT_ROOT.parent / 'src').exists():\n",
    "\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "\n",
    "\n",
    "from src.e0_ablation_utils import (\n",
    "\n",
    "    collect_versions,\n",
    "\n",
    "    get_project_paths,\n",
    "\n",
    "    get_torch_device,\n",
    "\n",
    "    set_global_seed,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "paths = get_project_paths(project_root=PROJECT_ROOT, output_dir='outputs/E0_ablation')\n",
    "\n",
    "DATA_PATH = paths.data_path\n",
    "\n",
    "OUTPUT_DIR = paths.output_dir\n",
    "\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "DEBUG = True  # [COMPLETAR: pon False para el run completo]\n",
    "\n",
    "\n",
    "\n",
    "seed_info = set_global_seed(SEED, deterministic=False)\n",
    "\n",
    "device, device_details = get_torch_device(prefer_cuda=True)\n",
    "\n",
    "\n",
    "\n",
    "print('PROJECT_ROOT:', PROJECT_ROOT)\n",
    "\n",
    "print('DATA_PATH:', DATA_PATH)\n",
    "\n",
    "print('OUTPUT_DIR:', OUTPUT_DIR)\n",
    "\n",
    "print('device:', device, device_details)\n",
    "\n",
    "print('seed:', seed_info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "id": "a6f7735d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T00:00:23.122442Z",
     "iopub.status.busy": "2026-02-01T00:00:23.122034Z",
     "iopub.status.idle": "2026-02-01T00:00:23.181112Z",
     "shell.execute_reply": "2026-02-01T00:00:23.179756Z",
     "shell.execute_reply.started": "2026-02-01T00:00:23.122420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded feature sets: ['FS0', 'FS1', 'FS2', 'COMPLETAR']\n",
      "FS0 n_features= 11\n",
      "FS1 n_features= 14\n",
      "FS2 n_features= 19\n"
     ]
    }
   ],
   "source": [
    "# Load feature set specification from notebook 01\n",
    "feature_sets_path = OUTPUT_DIR / 'feature_sets.json'\n",
    "if not feature_sets_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f'Missing {feature_sets_path}. Run 01_data_and_feature_sets.ipynb first.'\n",
    "    )\n",
    "\n",
    "feature_sets = json.loads(feature_sets_path.read_text(encoding='utf-8'))\n",
    "print('Loaded feature sets:', list(feature_sets.keys()))\n",
    "\n",
    "# quick peek\n",
    "for k in ['FS0','FS1','FS2']:\n",
    "    print(k, 'n_features=', len(feature_sets[k]['feature_cols']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dd12fd",
   "metadata": {},
   "source": [
    "## Configuración del experimento\n",
    "Ajusta hiperparámetros aquí. En modo `DEBUG=True`, se reducen epochs y/o se filtran tiendas para acelerar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "execution_state": "idle",
   "id": "dfc43b5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T00:00:23.182230Z",
     "iopub.status.busy": "2026-02-01T00:00:23.181898Z",
     "iopub.status.idle": "2026-02-01T00:00:23.314314Z",
     "shell.execute_reply": "2026-02-01T00:00:23.313218Z",
     "shell.execute_reply.started": "2026-02-01T00:00:23.182200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG stores: [1, 2, 3, 4, 5]\n",
      "df shape: (715, 8)\n",
      "date range: 2010-02-05 00:00:00 → 2012-10-26 00:00:00\n"
     ]
    }
   ],
   "source": [
    "from src.common import DEFAULT_LAGS, DEFAULT_ROLLINGS, EXOG_COLUMNS, load_data, make_features, temporal_split\n",
    "\n",
    "VAL_WEEKS = 8\n",
    "TEST_WEEKS = 39\n",
    "\n",
    "MODEL_SPECS = [\n",
    "    # (name, constructor)\n",
    "    ('lstm_exog', 'LSTMForecaster'),\n",
    "    ('transformer_exog', 'TransformerForecaster'),\n",
    "]\n",
    "\n",
    "# Deep model training params\n",
    "TRAINING_CFG = {\n",
    "    'lookback': 52,\n",
    "    'epochs': 3 if DEBUG else 20,\n",
    "    'batch_size': 64,\n",
    "    'lr': 1e-3,\n",
    "    'suppress_lookback_warning': False,\n",
    "}\n",
    "\n",
    "BASE_CFG = {\n",
    "    'lags': list(DEFAULT_LAGS),\n",
    "    'rollings': list(DEFAULT_ROLLINGS),\n",
    "}\n",
    "\n",
    "df = load_data(DATA_PATH)\n",
    "if DEBUG:\n",
    "    # speed-up: keep only a few stores\n",
    "    keep_stores = sorted(df['Store'].unique())[:5]\n",
    "    df = df[df['Store'].isin(keep_stores)].copy()\n",
    "    print('DEBUG stores:', keep_stores)\n",
    "\n",
    "print('df shape:', df.shape)\n",
    "print('date range:', df['Date'].min(), '→', df['Date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "execution_state": "idle",
   "id": "351c9d27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T00:00:23.315143Z",
     "iopub.status.busy": "2026-02-01T00:00:23.314901Z",
     "iopub.status.idle": "2026-02-01T00:00:23.328304Z",
     "shell.execute_reply": "2026-02-01T00:00:23.327269Z",
     "shell.execute_reply.started": "2026-02-01T00:00:23.315122Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_start': '2010-02-05', 'train_end': '2011-12-02', 'val_start': '2011-12-09', 'val_end': '2012-01-27', 'test_start': '2012-02-03', 'test_end': '2012-10-26'}\n",
      "train/val/test shapes: (480, 8) (40, 8) (195, 8)\n"
     ]
    }
   ],
   "source": [
    "# Split temporal global (train/val/test)\n",
    "train_raw, val_raw, test_raw, split_cfg = temporal_split(df, val_weeks=VAL_WEEKS, test_weeks=TEST_WEEKS)\n",
    "print(split_cfg.as_dict())\n",
    "\n",
    "# sanity: no overlap\n",
    "assert train_raw['Date'].max() < val_raw['Date'].min()\n",
    "assert val_raw['Date'].max() < test_raw['Date'].min()\n",
    "print('train/val/test shapes:', train_raw.shape, val_raw.shape, test_raw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3478dd",
   "metadata": {},
   "source": [
    "## Runner\n",
    "Ejecuta una combinación (modelo, feature set), guarda predicciones y métricas en `outputs/E0_ablation/<model>__<FS>/...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "execution_state": "idle",
   "id": "8d3f53c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T00:00:23.329343Z",
     "iopub.status.busy": "2026-02-01T00:00:23.329067Z",
     "iopub.status.idle": "2026-02-01T00:00:24.761584Z",
     "shell.execute_reply": "2026-02-01T00:00:24.760597Z",
     "shell.execute_reply.started": "2026-02-01T00:00:23.329315Z"
    }
   },
   "outputs": [],
   "source": [
    "from src.experiments import _compute_metrics_frames\n",
    "\n",
    "from src.models.lstm_forecaster import LSTMForecaster\n",
    "from src.models.transformer_forecaster import TransformerForecaster\n",
    "\n",
    "MODEL_CTORS = {\n",
    "    'LSTMForecaster': LSTMForecaster,\n",
    "    'TransformerForecaster': TransformerForecaster,\n",
    "}\n",
    "\n",
    "\n",
    "def run_one(model_label: str, model_ctor_name: str, fs_name: str) -> dict:\n",
    "    fs = feature_sets[fs_name]\n",
    "\n",
    "    # Build features for this FS (we still use make_features and then filter columns)\n",
    "    df_feat, all_cols = make_features(df, add_calendar=bool(fs['add_calendar']))\n",
    "    used_cols = list(fs['feature_cols'])\n",
    "\n",
    "    # Sanity: required columns exist\n",
    "    missing = [c for c in used_cols if c not in df_feat.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f'Missing engineered columns for {fs_name}: {missing[:10]}')\n",
    "\n",
    "    # Split again but on engineered df (same dates)\n",
    "    train_df = df_feat[df_feat['Date'].isin(train_raw['Date'].unique())].copy()\n",
    "    val_df = df_feat[df_feat['Date'].isin(val_raw['Date'].unique())].copy()\n",
    "    test_df = df_feat[df_feat['Date'].isin(test_raw['Date'].unique())].copy()\n",
    "\n",
    "    # Config passed to model\n",
    "    cfg = {\n",
    "        **BASE_CFG,\n",
    "        **TRAINING_CFG,\n",
    "        'add_calendar': bool(fs['add_calendar']),\n",
    "        'exog_cols': list(fs['exog_cols']),\n",
    "        'feature_cols': used_cols,\n",
    "    }\n",
    "\n",
    "    run_dir = OUTPUT_DIR / f'{model_label}__{fs_name}'\n",
    "    (run_dir / 'predictions').mkdir(parents=True, exist_ok=True)\n",
    "    (run_dir / 'metrics').mkdir(parents=True, exist_ok=True)\n",
    "    (run_dir / 'figures').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) Train -> predict val\n",
    "    t0 = time.time()\n",
    "    model = MODEL_CTORS[model_ctor_name]()\n",
    "    model.fit(train_df, cfg)\n",
    "    pred_val = model.predict(train_df, val_df, cfg)\n",
    "    val_sec = time.time() - t0\n",
    "\n",
    "    pred_val = pred_val.merge(\n",
    "        val_df[['Store','Date','Weekly_Sales']].rename(columns={'Weekly_Sales':'y_true'}),\n",
    "        on=['Store','Date'],\n",
    "        how='left',\n",
    "    )\n",
    "    pred_val = pred_val.rename(columns={'y_pred':'y_pred'})\n",
    "    pred_val['model'] = model_label\n",
    "    pred_val['feature_set'] = fs_name\n",
    "\n",
    "    mglob_val, mstore_val = _compute_metrics_frames(pred_val[['Store','Date','y_true','y_pred']], f'{model_label}__{fs_name}', group='VAL')\n",
    "    mglob_val['feature_set'] = fs_name\n",
    "    mglob_val['model'] = model_label\n",
    "\n",
    "    pred_val.to_csv(run_dir / 'predictions' / 'val_predictions.csv', index=False)\n",
    "    mglob_val.to_csv(run_dir / 'metrics' / 'val_metrics_global.csv', index=False)\n",
    "    mstore_val.to_csv(run_dir / 'metrics' / 'val_metrics_by_store.csv', index=False)\n",
    "\n",
    "    # 2) Train+Val -> predict test\n",
    "    t1 = time.time()\n",
    "    model2 = MODEL_CTORS[model_ctor_name]()\n",
    "    trainval_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "    model2.fit(trainval_df, cfg)\n",
    "    pred_test = model2.predict(trainval_df, test_df, cfg)\n",
    "    test_sec = time.time() - t1\n",
    "\n",
    "    pred_test = pred_test.merge(\n",
    "        test_df[['Store','Date','Weekly_Sales']].rename(columns={'Weekly_Sales':'y_true'}),\n",
    "        on=['Store','Date'],\n",
    "        how='left',\n",
    "    )\n",
    "    pred_test['model'] = model_label\n",
    "    pred_test['feature_set'] = fs_name\n",
    "\n",
    "    mglob_test, mstore_test = _compute_metrics_frames(pred_test[['Store','Date','y_true','y_pred']], f'{model_label}__{fs_name}', group='TEST')\n",
    "    mglob_test['feature_set'] = fs_name\n",
    "    mglob_test['model'] = model_label\n",
    "\n",
    "    pred_test.to_csv(run_dir / 'predictions' / 'test_predictions.csv', index=False)\n",
    "    mglob_test.to_csv(run_dir / 'metrics' / 'test_metrics_global.csv', index=False)\n",
    "    mstore_test.to_csv(run_dir / 'metrics' / 'test_metrics_by_store.csv', index=False)\n",
    "\n",
    "    # Save run metadata\n",
    "    meta = {\n",
    "        'seed': SEED,\n",
    "        'debug': DEBUG,\n",
    "        'model': model_label,\n",
    "        'feature_set': fs_name,\n",
    "        'split': split_cfg.as_dict(),\n",
    "        'config': cfg,\n",
    "        'device': device,\n",
    "        'device_details': device_details,\n",
    "        'versions': collect_versions(),\n",
    "        'timing_sec': {\n",
    "            'val_fit_predict': float(val_sec),\n",
    "            'test_fit_predict': float(test_sec),\n",
    "        },\n",
    "    }\n",
    "    (run_dir / 'run_metadata.json').write_text(json.dumps(meta, indent=2, ensure_ascii=False), encoding='utf-8')\n",
    "\n",
    "    return {\n",
    "        'model': model_label,\n",
    "        'feature_set': fs_name,\n",
    "        'val_fit_predict_sec': float(val_sec),\n",
    "        'test_fit_predict_sec': float(test_sec),\n",
    "        **{k: float(mglob_test.iloc[0][k]) for k in ['MAE','RMSE','sMAPE']},\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "execution_state": "idle",
   "id": "1e463991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-01T00:00:24.762354Z",
     "iopub.status.busy": "2026-02-01T00:00:24.761949Z",
     "iopub.status.idle": "2026-02-01T00:00:36.673596Z",
     "shell.execute_reply": "2026-02-01T00:00:36.672666Z",
     "shell.execute_reply.started": "2026-02-01T00:00:24.762331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== lstm_exog FS0 ===\n",
      "[LSTM] Reducing lookback from 52 to 43 due to limited per-store history (min len=44 after dropna).\n",
      "[LSTM] Reducing lookback from 52 to 51 due to limited per-store history (min len=52 after dropna).\n",
      "=== lstm_exog FS1 ===\n",
      "[LSTM] Reducing lookback from 52 to 43 due to limited per-store history (min len=44 after dropna).\n",
      "[LSTM] Reducing lookback from 52 to 51 due to limited per-store history (min len=52 after dropna).\n",
      "=== lstm_exog FS2 ===\n",
      "[LSTM] Reducing lookback from 52 to 43 due to limited per-store history (min len=44 after dropna).\n",
      "[LSTM] Reducing lookback from 52 to 51 due to limited per-store history (min len=52 after dropna).\n",
      "=== transformer_exog FS0 ===\n",
      "[Transformer] Reducing lookback from 52 to 43 due to limited per-store history (min len=44 after dropna).\n",
      "[Transformer] Reducing lookback from 52 to 51 due to limited per-store history (min len=52 after dropna).\n",
      "=== transformer_exog FS1 ===\n",
      "[Transformer] Reducing lookback from 52 to 43 due to limited per-store history (min len=44 after dropna).\n",
      "[Transformer] Reducing lookback from 52 to 51 due to limited per-store history (min len=52 after dropna).\n",
      "=== transformer_exog FS2 ===\n",
      "[Transformer] Reducing lookback from 52 to 43 due to limited per-store history (min len=44 after dropna).\n",
      "[Transformer] Reducing lookback from 52 to 51 due to limited per-store history (min len=52 after dropna).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>feature_set</th>\n",
       "      <th>val_fit_predict_sec</th>\n",
       "      <th>test_fit_predict_sec</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>sMAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lstm_exog</td>\n",
       "      <td>FS0</td>\n",
       "      <td>2.341277</td>\n",
       "      <td>0.318473</td>\n",
       "      <td>710089.292938</td>\n",
       "      <td>7.581639e+05</td>\n",
       "      <td>64.518722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lstm_exog</td>\n",
       "      <td>FS1</td>\n",
       "      <td>0.139632</td>\n",
       "      <td>0.324879</td>\n",
       "      <td>729429.159086</td>\n",
       "      <td>7.577307e+05</td>\n",
       "      <td>66.809316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>lstm_exog</td>\n",
       "      <td>FS2</td>\n",
       "      <td>0.148299</td>\n",
       "      <td>0.336044</td>\n",
       "      <td>722022.504484</td>\n",
       "      <td>7.552919e+05</td>\n",
       "      <td>66.000368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transformer_exog</td>\n",
       "      <td>FS0</td>\n",
       "      <td>0.307933</td>\n",
       "      <td>0.482765</td>\n",
       "      <td>920418.052308</td>\n",
       "      <td>1.088733e+06</td>\n",
       "      <td>118.051787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>transformer_exog</td>\n",
       "      <td>FS1</td>\n",
       "      <td>0.184602</td>\n",
       "      <td>0.488274</td>\n",
       "      <td>893289.744446</td>\n",
       "      <td>1.069371e+06</td>\n",
       "      <td>111.388046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>transformer_exog</td>\n",
       "      <td>FS2</td>\n",
       "      <td>0.193915</td>\n",
       "      <td>0.503371</td>\n",
       "      <td>611855.154668</td>\n",
       "      <td>6.413315e+05</td>\n",
       "      <td>58.701062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model feature_set  val_fit_predict_sec  test_fit_predict_sec  \\\n",
       "0         lstm_exog         FS0             2.341277              0.318473   \n",
       "1         lstm_exog         FS1             0.139632              0.324879   \n",
       "2         lstm_exog         FS2             0.148299              0.336044   \n",
       "3  transformer_exog         FS0             0.307933              0.482765   \n",
       "4  transformer_exog         FS1             0.184602              0.488274   \n",
       "5  transformer_exog         FS2             0.193915              0.503371   \n",
       "\n",
       "             MAE          RMSE       sMAPE  \n",
       "0  710089.292938  7.581639e+05   64.518722  \n",
       "1  729429.159086  7.577307e+05   66.809316  \n",
       "2  722022.504484  7.552919e+05   66.000368  \n",
       "3  920418.052308  1.088733e+06  118.051787  \n",
       "4  893289.744446  1.069371e+06  111.388046  \n",
       "5  611855.154668  6.413315e+05   58.701062  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /mnt/custom-file-systems/s3/shared/TFMAXEL/outputs/E0_ablation/summary_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "# Run grid\n",
    "results = []\n",
    "\n",
    "for model_label, ctor_name in MODEL_SPECS:\n",
    "    for fs_name in ['FS0','FS1','FS2']:\n",
    "        print('===', model_label, fs_name, '===')\n",
    "        row = run_one(model_label=model_label, model_ctor_name=ctor_name, fs_name=fs_name)\n",
    "        results.append(row)\n",
    "\n",
    "summary = pd.DataFrame(results).sort_values(['model','feature_set']).reset_index(drop=True)\n",
    "display(summary)\n",
    "\n",
    "summary_path = OUTPUT_DIR / 'summary_metrics.csv'\n",
    "summary.to_csv(summary_path, index=False)\n",
    "print('Saved:', summary_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf660c30",
   "metadata": {},
   "source": [
    "Siguiente: ejecutar **03_results_summary_and_plots.ipynb** para consolidación, deltas y visualizaciones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
