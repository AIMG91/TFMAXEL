{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e499a089",
   "metadata": {},
   "source": [
    "# 04 — LSTM global con exógenas confirmar\n",
    "\n",
    "**Objetivo:** forecasting de `Weekly_Sales` semanal por `Store` usando LSTM global con covariables exógenas.\n",
    "\n",
    "## Supuesto experimental (oracle exog)\n",
    "Se asume disponibilidad de todas las covariables exógenas durante el horizonte de predicción (escenario oracle).\n",
    "\n",
    "## Outputs estándar\n",
    "- `outputs/predictions/lstm_exog_predictions.csv` con: `Store, Date, y_true, y_pred, model`\n",
    "- `outputs/metrics/lstm_exog_metrics_global.csv`\n",
    "- `outputs/metrics/lstm_exog_metrics_by_store.csv`\n",
    "- `outputs/figures/lstm_exog_plot_*.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74d2be7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:32:36.271115Z",
     "iopub.status.busy": "2026-01-30T23:32:36.270752Z",
     "iopub.status.idle": "2026-01-30T23:32:36.648730Z",
     "shell.execute_reply": "2026-01-30T23:32:36.648105Z",
     "shell.execute_reply.started": "2026-01-30T23:32:36.271091Z"
    }
   },
   "outputs": [],
   "source": [
    "# 0) Imports y configuración\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.common import (\n",
    "    SplitConfig,\n",
    "    TEST_WEEKS,\n",
    "    compute_metrics,\n",
    "    load_data,\n",
    "    make_features,\n",
    "    save_outputs,\n",
    "    temporal_split,\n",
    " )\n",
    "\n",
    "MODEL_NAME = 'lstm_exog'\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = PROJECT_ROOT / 'data' / 'Walmart_Sales.csv'\n",
    "METADATA_PATH = PROJECT_ROOT / 'outputs' / 'metadata.json'\n",
    "OUTPUTS_DIR = PROJECT_ROOT / 'outputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f376ed20",
   "metadata": {},
   "source": [
    "## 1) Cargar metadata (split + features)\n",
    "Esto garantiza consistencia entre modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba822848",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:32:36.650765Z",
     "iopub.status.busy": "2026-01-30T23:32:36.650452Z",
     "iopub.status.idle": "2026-01-30T23:32:36.705553Z",
     "shell.execute_reply": "2026-01-30T23:32:36.705067Z",
     "shell.execute_reply.started": "2026-01-30T23:32:36.650746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split (metadata): {'train_start': '2010-02-05', 'train_end': '2011-12-02', 'val_start': '2011-12-09', 'val_end': '2012-01-27', 'test_start': '2012-02-03', 'test_end': '2012-10-26'}\n",
      "N features: 16\n"
     ]
    }
   ],
   "source": [
    "if METADATA_PATH.exists():\n",
    "    metadata = json.loads(METADATA_PATH.read_text(encoding='utf-8'))\n",
    "    split = metadata['split']\n",
    "    feature_cols = metadata['features']\n",
    "    split_cfg = SplitConfig(\n",
    "        train_start=pd.Timestamp(split['train_start']),\n",
    "        train_end=pd.Timestamp(split['train_end']),\n",
    "        val_start=pd.Timestamp(split['val_start']),\n",
    "        val_end=pd.Timestamp(split['val_end']),\n",
    "        test_start=pd.Timestamp(split['test_start']),\n",
    "        test_end=pd.Timestamp(split['test_end']),\n",
    "    )\n",
    "    print('Split (metadata):', split)\n",
    "    print('N features:', len(feature_cols))\n",
    "else:\n",
    "    metadata = None\n",
    "    split = None\n",
    "    feature_cols = None\n",
    "    split_cfg = None\n",
    "    print('metadata.json not found; will compute split with TEST_WEEKS=', TEST_WEEKS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2242b1",
   "metadata": {},
   "source": [
    "## 2) Carga de datos + features\n",
    "- Parseo/orden\n",
    "- Construcción de lags/rolling (sin leakage)\n",
    "- Exógenas alineadas por fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f5a599d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:32:36.706435Z",
     "iopub.status.busy": "2026-01-30T23:32:36.706194Z",
     "iopub.status.idle": "2026-01-30T23:32:36.812293Z",
     "shell.execute_reply": "2026-01-30T23:32:36.811727Z",
     "shell.execute_reply.started": "2026-01-30T23:32:36.706416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4095, 22)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data(DATA_PATH)\n",
    "df_feat, feature_cols_auto = make_features(df, add_calendar=True)\n",
    "if feature_cols is None:\n",
    "    feature_cols = feature_cols_auto\n",
    "\n",
    "# Importante: para entrenar, debes decidir cómo tratar NaNs creados por lags/rolling\n",
    "# Opción típica: descartar filas con NaNs en features (por store al inicio)\n",
    "model_df = df_feat.dropna(subset=feature_cols + ['Weekly_Sales']).copy()\n",
    "model_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b32f0b",
   "metadata": {},
   "source": [
    "## 3) Split temporal\n",
    "Reutiliza exactamente el split definido en el notebook 00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2b840e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:32:36.814123Z",
     "iopub.status.busy": "2026-01-30T23:32:36.813866Z",
     "iopub.status.idle": "2026-01-30T23:32:36.831731Z",
     "shell.execute_reply": "2026-01-30T23:32:36.831186Z",
     "shell.execute_reply.started": "2026-01-30T23:32:36.814104Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980 360 1755\n"
     ]
    }
   ],
   "source": [
    "if split_cfg is None:\n",
    "    train_df, val_df, test_df, split_cfg = temporal_split(df, test_weeks=TEST_WEEKS)\n",
    "else:\n",
    "    train_df = df[df['Date'].between(split_cfg.train_start, split_cfg.train_end)].copy()\n",
    "    val_df = df[df['Date'].between(split_cfg.val_start, split_cfg.val_end)].copy()\n",
    "    test_df = df[df['Date'].between(split_cfg.test_start, split_cfg.test_end)].copy()\n",
    "\n",
    "# Aplicar el split sobre model_df (ya sin NaNs por lags)\n",
    "train = model_df[model_df['Date'].between(split_cfg.train_start, split_cfg.train_end)].copy()\n",
    "val = model_df[model_df['Date'].between(split_cfg.val_start, split_cfg.val_end)].copy()\n",
    "test = model_df[model_df['Date'].between(split_cfg.test_start, split_cfg.test_end)].copy()\n",
    "\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5faddbe",
   "metadata": {},
   "source": [
    "## 4) Entrenamiento del modelo\n",
    "Implementación LSTM global con covariables exógenas.\n",
    "Incluye representaciones de Store (one-hot o embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4f2c21c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:32:36.832612Z",
     "iopub.status.busy": "2026-01-30T23:32:36.832370Z",
     "iopub.status.idle": "2026-01-30T23:32:36.835966Z",
     "shell.execute_reply": "2026-01-30T23:32:36.835418Z",
     "shell.execute_reply.started": "2026-01-30T23:32:36.832587Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: implementar entrenamiento LSTM con exógenas\n",
    "# Debe producir predicciones para TEST (ideal: también para VAL).\n",
    "y_pred_test = np.full(shape=len(test), fill_value=test['Weekly_Sales'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55a0b31",
   "metadata": {},
   "source": [
    "## 5) Métricas (MAE, RMSE, sMAPE)\n",
    "Se reporta:\n",
    "- Global\n",
    "- Por store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "956cd887",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:32:36.837344Z",
     "iopub.status.busy": "2026-01-30T23:32:36.836834Z",
     "iopub.status.idle": "2026-01-30T23:32:38.992937Z",
     "shell.execute_reply": "2026-01-30T23:32:38.992372Z",
     "shell.execute_reply.started": "2026-01-30T23:32:36.837323Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LSTM] WINDOW seleccionada automáticamente: 26 (train/val/test: 810/360/1755)\n"
     ]
    }
   ],
   "source": [
    "# Helper para construir features autoregresivas (sin leakage)\n",
    "\n",
    "\n",
    "def _compute_feature_vector(y_hist, date, exog_row, cfg):\n",
    "\n",
    "\n",
    "    lags = cfg[\"lags\"]\n",
    "\n",
    "\n",
    "    rollings = cfg[\"rollings\"]\n",
    "\n",
    "\n",
    "    add_calendar = bool(cfg.get(\"add_calendar\", True))\n",
    "\n",
    "\n",
    "    exog_cols = cfg[\"exog_cols\"]\n",
    "\n",
    "\n",
    "    feat = {}\n",
    "\n",
    "\n",
    "    for k in lags:\n",
    "\n",
    "\n",
    "        feat[f\"lag_{k}\"] = y_hist[-k] if len(y_hist) >= k else np.nan\n",
    "\n",
    "\n",
    "    for w in rollings:\n",
    "\n",
    "\n",
    "        if len(y_hist) >= w:\n",
    "\n",
    "\n",
    "            window = np.array(y_hist[-w:], dtype=float)\n",
    "\n",
    "\n",
    "            feat[f\"roll_mean_{w}\"] = float(window.mean())\n",
    "\n",
    "\n",
    "            feat[f\"roll_std_{w}\"] = float(window.std(ddof=0))\n",
    "\n",
    "\n",
    "        else:\n",
    "\n",
    "\n",
    "            feat[f\"roll_mean_{w}\"] = np.nan\n",
    "\n",
    "\n",
    "            feat[f\"roll_std_{w}\"] = np.nan\n",
    "\n",
    "\n",
    "    for c in exog_cols:\n",
    "\n",
    "\n",
    "        feat[c] = float(exog_row[c])\n",
    "\n",
    "\n",
    "    if add_calendar:\n",
    "\n",
    "\n",
    "        iso = pd.Timestamp(date).isocalendar()\n",
    "\n",
    "\n",
    "        feat[\"weekofyear\"] = int(iso.week)\n",
    "\n",
    "\n",
    "        feat[\"month\"] = int(pd.Timestamp(date).month)\n",
    "\n",
    "\n",
    "        feat[\"year\"] = int(pd.Timestamp(date).year)\n",
    "\n",
    "\n",
    "    vec = [feat.get(c, np.nan) for c in feature_cols]\n",
    "\n",
    "\n",
    "    return vec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ... resto del código previo ...\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from warnings import filterwarnings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "\n",
    "    import torch\n",
    "\n",
    "\n",
    "    from torch import nn\n",
    "\n",
    "\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "except Exception as exc:\n",
    "\n",
    "\n",
    "    raise ImportError(\"PyTorch no está instalado. Instala con: pip install torch\") from exc\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Configuración base + search space controlado\n",
    "\n",
    "\n",
    "EPOCHS_MAX = 200\n",
    "\n",
    "\n",
    "PATIENCE_ES = 15\n",
    "\n",
    "\n",
    "MIN_DELTA = 1e-4\n",
    "\n",
    "\n",
    "CLIP_NORM = 1.0\n",
    "\n",
    "\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "\n",
    "REDUCE_LR_FACTOR = 0.5\n",
    "\n",
    "\n",
    "REDUCE_LR_PATIENCE = 5\n",
    "\n",
    "\n",
    "EMB_DIM = 16\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Secuencia autoregresiva (ventana).\n",
    "\n",
    "\n",
    "# Nota: con splits por fecha, una ventana demasiado grande puede dejar el train vacío (num_samples=0).\n",
    "\n",
    "\n",
    "WINDOW_CANDIDATES = [52, 26, 13]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ventana fija: 26.\n",
    "# Si lo dejas en None, el notebook elegirá automáticamente la primera ventana viable.\n",
    "FORCE_WINDOW = 26  # ventana fija (pon None para auto)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Grilla pequeña (<=8 configs)\n",
    "\n",
    "\n",
    "lstm_search = [\n",
    "\n",
    "\n",
    "    {\"hidden_size\": 64, \"num_layers\": 2, \"dropout\": 0.1, \"lr\": 1e-3},\n",
    "\n",
    "\n",
    "    {\"hidden_size\": 64, \"num_layers\": 2, \"dropout\": 0.2, \"lr\": 3e-4},\n",
    "\n",
    "\n",
    "    {\"hidden_size\": 128, \"num_layers\": 2, \"dropout\": 0.1, \"lr\": 3e-4},\n",
    "\n",
    "\n",
    "    {\"hidden_size\": 128, \"num_layers\": 3, \"dropout\": 0.2, \"lr\": 1e-3},\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Anti-leakage: máscaras por fecha no deben solaparse\n",
    "\n",
    "\n",
    "assert split_cfg.train_end < split_cfg.val_start < split_cfg.val_end < split_cfg.test_start <= split_cfg.test_end\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Mapping Store -> idx para embeddings\n",
    "\n",
    "\n",
    "stores_sorted = sorted(model_df[\"Store\"].unique())\n",
    "\n",
    "\n",
    "store_to_idx = {s: i for i, s in enumerate(stores_sorted)}\n",
    "\n",
    "\n",
    "num_stores = len(store_to_idx)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "feature_cols_model = feature_cols  # lags/rolling/exógenas/calendario\n",
    "\n",
    "\n",
    "DEFAULT_LAGS = sorted({int(c.split(\"_\")[1]) for c in feature_cols if c.startswith(\"lag_\")})\n",
    "\n",
    "\n",
    "DEFAULT_ROLLINGS = sorted({int(c.split(\"_\")[2]) for c in feature_cols if c.startswith(\"roll_mean_\")})\n",
    "\n",
    "\n",
    "EXOG_COLUMNS = [c for c in feature_cols if not c.startswith(\"lag_\") and not c.startswith(\"roll_\")]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Escaladores SOLO con train\n",
    "\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "\n",
    "train_fit = train.copy()\n",
    "\n",
    "\n",
    "scaler_x.fit(train_fit[feature_cols_model].values)\n",
    "\n",
    "\n",
    "scaler_y.fit(train_fit[[\"Weekly_Sales\"]].values)\n",
    "\n",
    "\n",
    "assert hasattr(scaler_x, \"mean_\") and hasattr(scaler_y, \"mean_\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_sequences(df_in: pd.DataFrame, window: int):\n",
    "\n",
    "\n",
    "    sequences, targets, dates, stores_idx = [], [], [], []\n",
    "\n",
    "\n",
    "    df_in = df_in.sort_values([\"Store\", \"Date\"]).copy()\n",
    "\n",
    "\n",
    "    for store, g in df_in.groupby(\"Store\"):\n",
    "\n",
    "\n",
    "        g = g.sort_values(\"Date\")\n",
    "\n",
    "\n",
    "        X = scaler_x.transform(g[feature_cols_model].values)\n",
    "\n",
    "\n",
    "        y = scaler_y.transform(g[[\"Weekly_Sales\"]].values).ravel()\n",
    "\n",
    "\n",
    "        date_arr = g[\"Date\"].values\n",
    "\n",
    "\n",
    "        s_idx = store_to_idx[int(store)]\n",
    "\n",
    "\n",
    "        for t in range(window, len(g)):\n",
    "\n",
    "\n",
    "            sequences.append(X[t - window : t])\n",
    "\n",
    "\n",
    "            targets.append(y[t])\n",
    "\n",
    "\n",
    "            dates.append(date_arr[t])\n",
    "\n",
    "\n",
    "            stores_idx.append(s_idx)\n",
    "\n",
    "\n",
    "    return (\n",
    "\n",
    "\n",
    "        np.array(sequences),\n",
    "\n",
    "\n",
    "        np.array(targets),\n",
    "\n",
    "\n",
    "        np.array(dates),\n",
    "\n",
    "\n",
    "        np.array(stores_idx, dtype=int),\n",
    "\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _try_build_for_window(window: int):\n",
    "\n",
    "\n",
    "    X_seq, y_seq, d_seq, s_seq = build_sequences(model_df, window=window)\n",
    "\n",
    "\n",
    "    train_mask = (d_seq >= split_cfg.train_start) & (d_seq <= split_cfg.train_end)\n",
    "\n",
    "\n",
    "    val_mask = (d_seq >= split_cfg.val_start) & (d_seq <= split_cfg.val_end)\n",
    "\n",
    "\n",
    "    test_mask = (d_seq >= split_cfg.test_start) & (d_seq <= split_cfg.test_end)\n",
    "\n",
    "\n",
    "    return X_seq, y_seq, d_seq, s_seq, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Reporte de muestras por ventana (útil para justificar decisiones)\n",
    "\n",
    "\n",
    "window_report = []\n",
    "\n",
    "\n",
    "for w in WINDOW_CANDIDATES:\n",
    "\n",
    "\n",
    "    _X, _y, _d, _s, _tr, _va, _te = _try_build_for_window(w)\n",
    "\n",
    "\n",
    "    window_report.append({\"WINDOW\": w, \"n_train\": int(_tr.sum()), \"n_val\": int(_va.sum()), \"n_test\": int(_te.sum())})\n",
    "\n",
    "\n",
    "window_report_df = pd.DataFrame(window_report)\n",
    "\n",
    "\n",
    "display(window_report_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Selección de WINDOW\n",
    "\n",
    "\n",
    "if FORCE_WINDOW is not None:\n",
    "\n",
    "\n",
    "    WINDOW = int(FORCE_WINDOW)\n",
    "\n",
    "\n",
    "    if WINDOW not in WINDOW_CANDIDATES:\n",
    "\n",
    "\n",
    "        raise ValueError(f\"FORCE_WINDOW={WINDOW} no está en WINDOW_CANDIDATES={WINDOW_CANDIDATES}\")\n",
    "\n",
    "\n",
    "    row = window_report_df[window_report_df[\"WINDOW\"] == WINDOW].iloc[0]\n",
    "\n",
    "\n",
    "    if int(row.n_train) <= 0 or int(row.n_val) <= 0 or int(row.n_test) <= 0:\n",
    "\n",
    "\n",
    "        raise ValueError(\n",
    "\n",
    "            \"WINDOW forzada no es viable (alguna partición queda vacía). \"\n",
    "\n",
    "            f\"FORCE_WINDOW={WINDOW}; conteos: train/val/test={int(row.n_train)}/{int(row.n_val)}/{int(row.n_test)}\"\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "    print(f\"[LSTM] WINDOW forzada: {WINDOW} (train/val/test: {int(row.n_train)}/{int(row.n_val)}/{int(row.n_test)})\")\n",
    "\n",
    "\n",
    "else:\n",
    "\n",
    "\n",
    "    WINDOW = None\n",
    "\n",
    "\n",
    "    for w in WINDOW_CANDIDATES:\n",
    "\n",
    "\n",
    "        row = window_report_df[window_report_df[\"WINDOW\"] == w].iloc[0]\n",
    "\n",
    "\n",
    "        if int(row.n_train) > 0 and int(row.n_val) > 0 and int(row.n_test) > 0:\n",
    "\n",
    "\n",
    "            WINDOW = int(w)\n",
    "\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "    if WINDOW is None:\n",
    "\n",
    "\n",
    "        msg = (\n",
    "\n",
    "            \"No hay muestras tras construir secuencias para ninguna WINDOW candidata.\\n\"\n",
    "\n",
    "            f\"Candidatas: {WINDOW_CANDIDATES}\\n\"\n",
    "\n",
    "            f\"Split train: {split_cfg.train_start} -> {split_cfg.train_end}\\n\"\n",
    "\n",
    "            f\"Split val:   {split_cfg.val_start} -> {split_cfg.val_end}\\n\"\n",
    "\n",
    "            f\"Split test:  {split_cfg.test_start} -> {split_cfg.test_end}\\n\"\n",
    "\n",
    "            \"Sugerencia: reduce WINDOW, revisa que Date sea datetime64, o ajusta el split.\"\n",
    "\n",
    "        )\n",
    "\n",
    "        raise ValueError(msg)\n",
    "\n",
    "\n",
    "\n",
    "    row = window_report_df[window_report_df[\"WINDOW\"] == WINDOW].iloc[0]\n",
    "\n",
    "    print(f\"[LSTM] WINDOW seleccionada automáticamente: {WINDOW} (train/val/test: {int(row.n_train)}/{int(row.n_val)}/{int(row.n_test)})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Recalcular arrays con la WINDOW elegida\n",
    "\n",
    "\n",
    "X_seq, y_seq, d_seq, s_seq, train_mask, val_mask, test_mask = _try_build_for_window(WINDOW)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Split por fecha (sin leakage)\n",
    "\n",
    "\n",
    "assert not (train_mask & val_mask).any() and not (train_mask & test_mask).any() and not (val_mask & test_mask).any()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train, s_train = X_seq[train_mask], y_seq[train_mask], s_seq[train_mask]\n",
    "\n",
    "\n",
    "X_val, y_val, s_val = X_seq[val_mask], y_seq[val_mask], s_seq[val_mask]\n",
    "\n",
    "\n",
    "X_test_seq, y_test_seq, s_test_seq = X_seq[test_mask], y_seq[test_mask], s_seq[test_mask]\n",
    "\n",
    "\n",
    "test_dates = d_seq[test_mask]\n",
    "\n",
    "\n",
    "test_stores = s_seq[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if len(X_train) == 0:\n",
    "\n",
    "\n",
    "    raise ValueError(\"Train vacío tras construir secuencias. Revisa WINDOW y split_cfg.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "\n",
    "    TensorDataset(\n",
    "\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "\n",
    "        torch.tensor(s_train, dtype=torch.long),\n",
    "\n",
    "        torch.tensor(y_train, dtype=torch.float32),\n",
    "\n",
    "    ),\n",
    "\n",
    "    batch_size=64,\n",
    "\n",
    "    shuffle=True,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "val_loader = DataLoader(\n",
    "\n",
    "    TensorDataset(\n",
    "\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "\n",
    "        torch.tensor(s_val, dtype=torch.long),\n",
    "\n",
    "        torch.tensor(y_val, dtype=torch.float32),\n",
    "\n",
    "    ),\n",
    "\n",
    "    batch_size=64,\n",
    "\n",
    "    shuffle=False,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, hidden_size: int, num_layers: int, dropout: float, emb_dim: int, num_stores: int):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.store_emb = nn.Embedding(num_stores, emb_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size + emb_dim, hidden_size, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, store_idx):\n",
    "\n",
    "        emb = self.store_emb(store_idx)\n",
    "\n",
    "        emb_expanded = emb.unsqueeze(1).expand(-1, x.size(1), -1)\n",
    "\n",
    "        x_cat = torch.cat([x, emb_expanded], dim=-1)\n",
    "\n",
    "        out, _ = self.lstm(x_cat)\n",
    "\n",
    "        last = out[:, -1, :]\n",
    "\n",
    "        return self.fc(last).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "def train_eval(config_run: dict):\n",
    "\n",
    "    model = LSTMRegressor(\n",
    "\n",
    "        input_size=X_train.shape[-1],\n",
    "\n",
    "        hidden_size=config_run[\"hidden_size\"],\n",
    "\n",
    "        num_layers=config_run[\"num_layers\"],\n",
    "\n",
    "        dropout=config_run[\"dropout\"],\n",
    "\n",
    "        emb_dim=EMB_DIM,\n",
    "\n",
    "        num_stores=num_stores,\n",
    "\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa5b457",
   "metadata": {},
   "source": [
    "## 5) Métricas (MAE, RMSE, sMAPE)\n",
    "Se reporta:\n",
    "- Global\n",
    "- Por store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15f92514",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:32:38.993854Z",
     "iopub.status.busy": "2026-01-30T23:32:38.993607Z",
     "iopub.status.idle": "2026-01-30T23:32:39.015508Z",
     "shell.execute_reply": "2026-01-30T23:32:39.014967Z",
     "shell.execute_reply.started": "2026-01-30T23:32:38.993834Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       model            MAE           RMSE      sMAPE      WAPE\n",
       " 0  lstm_exog  458561.873194  541160.031628  46.465994  0.439471,\n",
       "        model  Store           MAE          RMSE       sMAPE      WAPE\n",
       " 0  lstm_exog      1  5.585687e+05  5.681535e+05   41.991265  0.348668\n",
       " 1  lstm_exog      2  8.688612e+05  8.742111e+05   58.643952  0.454354\n",
       " 2  lstm_exog      3  6.192487e+05  6.197298e+05   84.464654  1.459831\n",
       " 3  lstm_exog      4  1.132147e+06  1.135903e+06   70.237988  0.520387\n",
       " 4  lstm_exog      5  7.105738e+05  7.108503e+05  103.320071  2.134709)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df = pd.DataFrame({\n",
    "    'Store': test['Store'].astype(int).values,\n",
    "    'Date': test['Date'].values,\n",
    "    'y_true': test['Weekly_Sales'].values,\n",
    "    'y_pred': np.asarray(y_pred_test, dtype=float),\n",
    "    'model': MODEL_NAME,\n",
    "})\n",
    "\n",
    "global_metrics = compute_metrics(pred_df['y_true'].values, pred_df['y_pred'].values)\n",
    "metrics_global_df = pd.DataFrame([{'model': MODEL_NAME, **global_metrics}])\n",
    "\n",
    "by_store = []\n",
    "for store, g in pred_df.groupby('Store'):\n",
    "    m = compute_metrics(g['y_true'].values, g['y_pred'].values)\n",
    "    by_store.append({'model': MODEL_NAME, 'Store': int(store), **m})\n",
    "metrics_by_store_df = pd.DataFrame(by_store).sort_values('Store')\n",
    "\n",
    "metrics_global_df, metrics_by_store_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39e50d",
   "metadata": {},
   "source": [
    "## 6) Guardado de outputs estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d9ec213",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:32:39.016450Z",
     "iopub.status.busy": "2026-01-30T23:32:39.016195Z",
     "iopub.status.idle": "2026-01-30T23:32:39.292348Z",
     "shell.execute_reply": "2026-01-30T23:32:39.291830Z",
     "shell.execute_reply.started": "2026-01-30T23:32:39.016431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': 'c:\\\\Users\\\\usuario\\\\Documents\\\\Master AI\\\\TFM\\\\MEMORIA 2.0\\\\outputs\\\\predictions\\\\lstm_exog_predictions.csv',\n",
       " 'metrics_global': 'c:\\\\Users\\\\usuario\\\\Documents\\\\Master AI\\\\TFM\\\\MEMORIA 2.0\\\\outputs\\\\metrics\\\\lstm_exog_metrics_global.csv',\n",
       " 'metrics_by_store': 'c:\\\\Users\\\\usuario\\\\Documents\\\\Master AI\\\\TFM\\\\MEMORIA 2.0\\\\outputs\\\\metrics\\\\lstm_exog_metrics_by_store.csv'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = save_outputs(\n",
    "    model_name=MODEL_NAME,\n",
    "    predictions=pred_df,\n",
    "    metrics_global=metrics_global_df,\n",
    "    metrics_by_store=metrics_by_store_df,\n",
    "    output_dir=OUTPUTS_DIR,\n",
    ")\n",
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b2d7eb",
   "metadata": {},
   "source": [
    "## 7) Figuras\n",
    "- 3 tiendas: real vs predicción en test\n",
    "- Distribución del error (`y_true - y_pred`)\n",
    "\n",
    "Guardar PNGs en `outputs/figures/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65372443",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:32:39.293467Z",
     "iopub.status.busy": "2026-01-30T23:32:39.293126Z",
     "iopub.status.idle": "2026-01-30T23:32:41.580868Z",
     "shell.execute_reply": "2026-01-30T23:32:41.579918Z",
     "shell.execute_reply.started": "2026-01-30T23:32:39.293440Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "FIG_DIR = OUTPUTS_DIR / \"figures\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Selección de 3 tiendas (mayor media de ventas en test)\n",
    "top_stores = (\n",
    "    pred_df.groupby(\"Store\")[\"y_true\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(3)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "for store in top_stores:\n",
    "    g = pred_df[pred_df[\"Store\"] == store].sort_values(\"Date\")\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(g[\"Date\"], g[\"y_true\"], label=\"y_true\")\n",
    "    plt.plot(g[\"Date\"], g[\"y_pred\"], label=\"y_pred\")\n",
    "    plt.title(f\"Store {store} — LSTM\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Weekly_Sales\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{MODEL_NAME}_plot_store_{store}.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# Distribución de error\n",
    "errors = pred_df[\"y_true\"] - pred_df[\"y_pred\"]\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(errors, bins=30, kde=True)\n",
    "plt.title(\"Error distribution (y_true - y_pred)\")\n",
    "plt.xlabel(\"Error\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / f\"{MODEL_NAME}_plot_error_dist.png\", dpi=150)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1dbdf83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:32:41.583975Z",
     "iopub.status.busy": "2026-01-30T23:32:41.583507Z",
     "iopub.status.idle": "2026-01-30T23:32:41.605980Z",
     "shell.execute_reply": "2026-01-30T23:32:41.605178Z",
     "shell.execute_reply.started": "2026-01-30T23:32:41.583942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>sMAPE</th>\n",
       "      <th>WAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lstm_exog</td>\n",
       "      <td>458561.873194</td>\n",
       "      <td>541160.031628</td>\n",
       "      <td>46.465994</td>\n",
       "      <td>0.439471</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model            MAE           RMSE      sMAPE      WAPE\n",
       "0  lstm_exog  458561.873194  541160.031628  46.465994  0.439471"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reconstruir results_df si no está en memoria\n",
    "import pandas as pd\n",
    "\n",
    "if \"results_df\" not in globals():\n",
    "    if \"results\" in globals() and len(results) > 0:\n",
    "        rows = []\n",
    "        for r in results:\n",
    "            row = {k: v for k, v in r.items() if k not in {\"state_dict\", \"model_state\"}}\n",
    "            cfg = row.pop(\"config\", {})\n",
    "            if isinstance(cfg, dict):\n",
    "                row.update(cfg)\n",
    "            rows.append(row)\n",
    "        results_df = pd.DataFrame(rows)\n",
    "    elif \"metrics_global_df\" in globals():\n",
    "        # Fallback: usar métricas globales si no hubo búsqueda\n",
    "        results_df = metrics_global_df.copy()\n",
    "    else:\n",
    "        raise ValueError(\"No hay datos para construir results_df; vuelve a correr la celda de entrenamiento.\")\n",
    "\n",
    "if not results_df.empty and all(col in results_df.columns for col in [\"sMAPE\", \"MAE\"]):\n",
    "    results_df = results_df.sort_values([\"sMAPE\", \"MAE\"]).reset_index(drop=True)\n",
    "\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "038802e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:32:41.607438Z",
     "iopub.status.busy": "2026-01-30T23:32:41.607094Z",
     "iopub.status.idle": "2026-01-30T23:32:41.673288Z",
     "shell.execute_reply": "2026-01-30T23:32:41.672641Z",
     "shell.execute_reply.started": "2026-01-30T23:32:41.607409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor config (ordenada por sMAPE):\n",
      "model        lstm_exog\n",
      "MAE      458561.873194\n",
      "RMSE     541160.031628\n",
      "sMAPE        46.465994\n",
      "WAPE          0.439471\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Baseline (EPOCHS=20) encontrado:\n",
      "       model            MAE           RMSE      sMAPE      WAPE\n",
      "0  lstm_exog  458561.873194  541160.031628  46.465994  0.439471\n",
      "Comparación sMAPE delta: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Resumen rápido: mejor config y (si existe) baseline de 20 epochs\n",
    "\n",
    "best_row = results_df.iloc[0]\n",
    "\n",
    "print(\"Mejor config (ordenada por sMAPE):\")\n",
    "\n",
    "print(best_row)\n",
    "\n",
    "\n",
    "\n",
    "import pathlib\n",
    "\n",
    "baseline_path = pathlib.Path(OUTPUTS_DIR) / \"metrics\" / \"lstm_exog_metrics_global.csv\"\n",
    "\n",
    "if baseline_path.exists():\n",
    "\n",
    "    baseline_df = pd.read_csv(baseline_path)\n",
    "\n",
    "    print(\"\\nBaseline (EPOCHS=20) encontrado:\")\n",
    "\n",
    "    print(baseline_df)\n",
    "\n",
    "    print(\"Comparación sMAPE delta:\", float(best_row[\"sMAPE\"]) - float(baseline_df.loc[0, \"sMAPE\"]))\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"\\nBaseline (EPOCHS=20) no encontrado; ejecuta el baseline para comparar.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
