{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55663c64",
   "metadata": {},
   "source": [
    "# 03 — GluonTS DeepAR con exógenas (global multi-serie)\n",
    "\n",
    "**Objetivo:** forecasting de `Weekly_Sales` semanal por `Store` usando DeepAR con covariables exógenas.\n",
    "\n",
    "## Supuesto experimental (oracle exog)\n",
    "Se asume disponibilidad de todas las covariables exógenas durante el horizonte de predicción (escenario oracle).\n",
    "\n",
    "## Outputs estándar\n",
    "- `outputs/predictions/deepar_exog_predictions.csv` con: `Store, Date, y_true, y_pred, model`\n",
    "- `outputs/metrics/deepar_exog_metrics_global.csv`\n",
    "- `outputs/metrics/deepar_exog_metrics_by_store.csv`\n",
    "- `outputs/figures/deepar_exog_plot_*.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c706ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0) Imports y configuración\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.common import (\n",
    "    compute_metrics,\n",
    "    load_data,\n",
    "    make_features,\n",
    "    save_outputs,\n",
    "    temporal_split,\n",
    ")\n",
    "\n",
    "MODEL_NAME = 'deepar_exog'\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = PROJECT_ROOT / 'data' / 'Walmart_Sales.csv'\n",
    "METADATA_PATH = PROJECT_ROOT / 'outputs' / 'metadata.json'\n",
    "OUTPUTS_DIR = PROJECT_ROOT / 'outputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49851fc",
   "metadata": {},
   "source": [
    "## 1) Cargar metadata (split + features)\n",
    "Esto garantiza consistencia entre modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8c36299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: {'train_start': '2010-02-05', 'train_end': '2012-07-06', 'val_start': '2012-07-13', 'val_end': '2012-08-31', 'test_start': '2012-09-07', 'test_end': '2012-10-26'}\n",
      "N features: 16\n"
     ]
    }
   ],
   "source": [
    "metadata = json.loads(METADATA_PATH.read_text(encoding='utf-8'))\n",
    "split = metadata['split']\n",
    "feature_cols = metadata['features']\n",
    "print('Split:', split)\n",
    "print('N features:', len(feature_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc5890",
   "metadata": {},
   "source": [
    "## 2) Carga de datos + features\n",
    "- Parseo/orden\n",
    "- Construcción de lags/rolling (sin leakage)\n",
    "- Exógenas alineadas por fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90ca2f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4095, 22)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data(DATA_PATH)\n",
    "df_feat, _ = make_features(df, add_calendar=True)\n",
    "\n",
    "# Importante: para entrenar, debes decidir cómo tratar NaNs creados por lags/rolling\n",
    "# Opción típica: descartar filas con NaNs en features (por store al inicio)\n",
    "model_df = df_feat.dropna(subset=feature_cols + ['Weekly_Sales']).copy()\n",
    "model_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b40169f",
   "metadata": {},
   "source": [
    "## 3) Split temporal\n",
    "Reutiliza exactamente el split definido en el notebook 00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8562e9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3375 360 360\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df, split_cfg = temporal_split(df)\n",
    "\n",
    "# Aplicar el split sobre model_df (ya sin NaNs por lags)\n",
    "train = model_df[model_df['Date'].between(split_cfg.train_start, split_cfg.train_end)].copy()\n",
    "val = model_df[model_df['Date'].between(split_cfg.val_start, split_cfg.val_end)].copy()\n",
    "test = model_df[model_df['Date'].between(split_cfg.test_start, split_cfg.test_end)].copy()\n",
    "\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf6687",
   "metadata": {},
   "source": [
    "## 4) Entrenamiento del modelo\n",
    "Implementación DeepAR global multi-serie con covariables exógenas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b63759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implementar entrenamiento DeepAR con exógenas\n",
    "# Debe producir predicciones para TEST (ideal: también para VAL).\n",
    "y_pred_test = np.full(shape=len(test), fill_value=test['Weekly_Sales'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86593e0e",
   "metadata": {},
   "source": [
    "## 5) Métricas (MAE, RMSE, sMAPE)\n",
    "Se reporta:\n",
    "- Global\n",
    "- Por store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c7da9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== DeepAR config {'hidden_size': 40, 'num_layers': 2, 'dropout_rate': 0.1, 'lr': 0.001, 'batch_size': 32} ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type        | Params | In sizes | Out sizes  \n",
      "---------------------------------------------------------------\n",
      "0 | model | DeepARModel | 25.1 K | ?        | [1, 100, 8]\n",
      "---------------------------------------------------------------\n",
      "25.1 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.1 K    Total params\n",
      "0.100     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 52it [00:03, 16.16it/s, loss=10.8, v_num=43, val_loss=13.00, train_loss=11.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'val_loss' reached 13.01339 (best 13.01339), saving model to 'c:\\\\Users\\\\usuario\\\\Documents\\\\Master AI\\\\TFM\\\\MEMORIA 2.0\\\\notebooks\\\\lightning_logs\\\\version_43\\\\checkpoints\\\\epoch=0-step=50.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: : 52it [00:03, 15.90it/s, loss=10.8, v_num=43, val_loss=12.90, train_loss=10.70]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 100: 'val_loss' reached 12.89910 (best 12.89910), saving model to 'c:\\\\Users\\\\usuario\\\\Documents\\\\Master AI\\\\TFM\\\\MEMORIA 2.0\\\\notebooks\\\\lightning_logs\\\\version_43\\\\checkpoints\\\\epoch=1-step=100.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: : 52it [00:03, 16.86it/s, loss=10.6, v_num=43, val_loss=12.90, train_loss=10.60]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 150: 'val_loss' reached 12.88313 (best 12.88313), saving model to 'c:\\\\Users\\\\usuario\\\\Documents\\\\Master AI\\\\TFM\\\\MEMORIA 2.0\\\\notebooks\\\\lightning_logs\\\\version_43\\\\checkpoints\\\\epoch=2-step=150.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: : 52it [00:03, 16.60it/s, loss=10.5, v_num=43, val_loss=12.90, train_loss=10.50]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 200: 'val_loss' reached 12.88148 (best 12.88148), saving model to 'c:\\\\Users\\\\usuario\\\\Documents\\\\Master AI\\\\TFM\\\\MEMORIA 2.0\\\\notebooks\\\\lightning_logs\\\\version_43\\\\checkpoints\\\\epoch=3-step=200.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: : 52it [00:03, 16.67it/s, loss=10.5, v_num=43, val_loss=12.90, train_loss=10.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 250: 'val_loss' reached 12.86806 (best 12.86806), saving model to 'c:\\\\Users\\\\usuario\\\\Documents\\\\Master AI\\\\TFM\\\\MEMORIA 2.0\\\\notebooks\\\\lightning_logs\\\\version_43\\\\checkpoints\\\\epoch=4-step=250.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: : 52it [00:03, 15.82it/s, loss=10.3, v_num=43, val_loss=12.90, train_loss=10.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 300: 'val_loss' reached 12.86639 (best 12.86639), saving model to 'c:\\\\Users\\\\usuario\\\\Documents\\\\Master AI\\\\TFM\\\\MEMORIA 2.0\\\\notebooks\\\\lightning_logs\\\\version_43\\\\checkpoints\\\\epoch=5-step=300.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: : 52it [00:03, 14.25it/s, loss=10.6, v_num=43, val_loss=12.90, train_loss=10.40]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 350: 'val_loss' reached 12.86351 (best 12.86351), saving model to 'c:\\\\Users\\\\usuario\\\\Documents\\\\Master AI\\\\TFM\\\\MEMORIA 2.0\\\\notebooks\\\\lightning_logs\\\\version_43\\\\checkpoints\\\\epoch=6-step=350.ckpt' as top 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: : 25it [00:03,  8.14it/s, loss=10.6, v_num=43, val_loss=12.90, train_loss=10.40]"
     ]
    }
   ],
   "source": [
    "from warnings import filterwarnings\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    from gluonts.dataset.common import ListDataset\n",
    "\n",
    "    from gluonts.dataset.field_names import FieldName\n",
    "\n",
    "    from gluonts.torch.model.deepar import DeepAREstimator\n",
    "\n",
    "except Exception as exc:\n",
    "\n",
    "    raise ImportError(\"GluonTS no está instalado. Instala con: pip install gluonts\") from exc\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "except Exception as exc:\n",
    "\n",
    "    raise ImportError(\"pytorch_lightning es necesario para early stopping en GluonTS\") from exc\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "# Anti-leakage\n",
    "\n",
    "assert split_cfg.train_end < split_cfg.val_start < split_cfg.val_end < split_cfg.test_start <= split_cfg.test_end\n",
    "\n",
    "\n",
    "\n",
    "# DeepAR usa solo exógenas (sin lags/rolling)\n",
    "\n",
    "deepar_exog_cols = [c for c in feature_cols if not c.startswith(\"lag_\") and not c.startswith(\"roll_\")]\n",
    "\n",
    "\n",
    "\n",
    "# Dataset completo con exógenas (oracle exog permitida)\n",
    "\n",
    "df_full, _ = make_features(df, add_calendar=True)\n",
    "\n",
    "\n",
    "\n",
    "# Fechas del split\n",
    "\n",
    "train_end = split_cfg.train_end\n",
    "\n",
    "val_end = split_cfg.val_end\n",
    "\n",
    "test_start = split_cfg.test_start\n",
    "\n",
    "test_end = split_cfg.test_end\n",
    "\n",
    "\n",
    "\n",
    "# Frecuencia semanal\n",
    "\n",
    "dates_all = df_full.sort_values(\"Date\")[\"Date\"].drop_duplicates().values\n",
    "\n",
    "freq = pd.infer_freq(pd.to_datetime(dates_all)) or \"W-FRI\"\n",
    "\n",
    "\n",
    "\n",
    "# Horizonte de predicción\n",
    "\n",
    "test_dates = pd.date_range(start=test_start, end=test_end, freq=freq)\n",
    "\n",
    "prediction_length = len(test_dates)\n",
    "\n",
    "\n",
    "\n",
    "def build_series(store_df: pd.DataFrame) -> tuple[np.ndarray, np.ndarray, pd.Timestamp]:\n",
    "\n",
    "    store_df = store_df.sort_values(\"Date\")\n",
    "\n",
    "    y = store_df[\"Weekly_Sales\"].values.astype(float)\n",
    "\n",
    "    exog = store_df[deepar_exog_cols].values.astype(float).T\n",
    "\n",
    "    start = pd.Timestamp(store_df[\"Date\"].iloc[0])\n",
    "\n",
    "    return y, exog, start\n",
    "\n",
    "\n",
    "\n",
    "# Grilla controlada (<=8 configs)\n",
    "\n",
    "deepar_search = [\n",
    "\n",
    "    {\"hidden_size\": 40, \"num_layers\": 2, \"dropout_rate\": 0.1, \"lr\": 1e-3, \"batch_size\": 32},\n",
    "\n",
    "    {\"hidden_size\": 40, \"num_layers\": 3, \"dropout_rate\": 0.2, \"lr\": 5e-4, \"batch_size\": 64},\n",
    "\n",
    "    {\"hidden_size\": 80, \"num_layers\": 2, \"dropout_rate\": 0.1, \"lr\": 5e-4, \"batch_size\": 32},\n",
    "\n",
    "    {\"hidden_size\": 80, \"num_layers\": 3, \"dropout_rate\": 0.2, \"lr\": 1e-3, \"batch_size\": 64},\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "for cfg in deepar_search:\n",
    "\n",
    "    print(f\"\\n=== DeepAR config {cfg} ===\")\n",
    "\n",
    "\n",
    "\n",
    "    # Dataset de entrenamiento: target solo hasta train_end\n",
    "\n",
    "    train_records = []\n",
    "\n",
    "    for store, g in df_full[df_full[\"Date\"] <= train_end].groupby(\"Store\"):\n",
    "\n",
    "        y, exog, start = build_series(g)\n",
    "\n",
    "        train_records.append({\n",
    "\n",
    "            FieldName.TARGET: y,\n",
    "\n",
    "            FieldName.START: start,\n",
    "\n",
    "            FieldName.FEAT_DYNAMIC_REAL: exog,\n",
    "\n",
    "            FieldName.ITEM_ID: str(int(store)),\n",
    "\n",
    "        })\n",
    "\n",
    "    train_ds = ListDataset(train_records, freq=freq)\n",
    "\n",
    "\n",
    "\n",
    "    # Dataset de validación para early stopping: target hasta val_end\n",
    "\n",
    "    val_records = []\n",
    "\n",
    "    for store, g in df_full[df_full[\"Date\"] <= val_end].groupby(\"Store\"):\n",
    "\n",
    "        y, exog, start = build_series(g)\n",
    "\n",
    "        val_records.append({\n",
    "\n",
    "            FieldName.TARGET: y,\n",
    "\n",
    "            FieldName.START: start,\n",
    "\n",
    "            FieldName.FEAT_DYNAMIC_REAL: exog,\n",
    "\n",
    "            FieldName.ITEM_ID: str(int(store)),\n",
    "\n",
    "        })\n",
    "\n",
    "    val_ds = ListDataset(val_records, freq=freq)\n",
    "\n",
    "\n",
    "\n",
    "    # Dataset para predicción: target hasta train_end + exógenas completas hasta test_end\n",
    "\n",
    "    pred_records = []\n",
    "\n",
    "    for store, g in df_full[df_full[\"Date\"] <= test_end].groupby(\"Store\"):\n",
    "\n",
    "        g_train = g[g[\"Date\"] <= train_end]\n",
    "\n",
    "        if g_train.empty:\n",
    "\n",
    "            continue\n",
    "\n",
    "        y, _, start = build_series(g_train)\n",
    "\n",
    "        g_full = g.sort_values(\"Date\")\n",
    "\n",
    "        max_len = len(g_train) + prediction_length\n",
    "\n",
    "        exog_full = g_full[deepar_exog_cols].iloc[:max_len].values.astype(float).T\n",
    "\n",
    "        pred_records.append({\n",
    "\n",
    "            FieldName.TARGET: y,\n",
    "\n",
    "            FieldName.START: start,\n",
    "\n",
    "            FieldName.FEAT_DYNAMIC_REAL: exog_full,\n",
    "\n",
    "            FieldName.ITEM_ID: str(int(store)),\n",
    "\n",
    "        })\n",
    "\n",
    "    pred_ds = ListDataset(pred_records, freq=freq)\n",
    "\n",
    "\n",
    "\n",
    "    estimator = DeepAREstimator(\n",
    "\n",
    "        prediction_length=prediction_length,\n",
    "\n",
    "        context_length=52,\n",
    "\n",
    "        freq=freq,\n",
    "\n",
    "        num_feat_dynamic_real=len(deepar_exog_cols),\n",
    "\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "\n",
    "        num_batches_per_epoch=50,\n",
    "\n",
    "        lr=cfg[\"lr\"],\n",
    "\n",
    "        num_layers=cfg[\"num_layers\"],\n",
    "\n",
    "        hidden_size=cfg[\"hidden_size\"],\n",
    "\n",
    "        dropout_rate=cfg[\"dropout_rate\"],\n",
    "\n",
    "        scaling=True,\n",
    "\n",
    "        num_parallel_samples=100,\n",
    "\n",
    "        trainer_kwargs={\n",
    "\n",
    "            \"max_epochs\": 200,\n",
    "\n",
    "            \"callbacks\": [EarlyStopping(monitor=\"val_loss\", patience=15, min_delta=1e-4, mode=\"min\")],\n",
    "\n",
    "        },\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    predictor = estimator.train(training_data=train_ds, validation_data=val_ds)\n",
    "\n",
    "\n",
    "\n",
    "    store_preds = {}\n",
    "\n",
    "    for forecast, item in zip(predictor.predict(pred_ds), pred_records):\n",
    "\n",
    "        store_id = int(item[FieldName.ITEM_ID])\n",
    "\n",
    "        store_preds[store_id] = forecast.mean\n",
    "\n",
    "\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for store, g in test.groupby(\"Store\"):\n",
    "\n",
    "        g = g.sort_values(\"Date\")\n",
    "\n",
    "        yhat = store_preds.get(int(store))\n",
    "\n",
    "        if yhat is None:\n",
    "\n",
    "            yhat = np.full(len(g), train[\"Weekly_Sales\"].mean())\n",
    "\n",
    "        else:\n",
    "\n",
    "            yhat = np.asarray(yhat)[: len(g)]\n",
    "\n",
    "            if len(yhat) < len(g):\n",
    "\n",
    "                yhat = np.pad(yhat, (0, len(g) - len(yhat)), constant_values=yhat[-1])\n",
    "\n",
    "        preds.append(\n",
    "\n",
    "            pd.DataFrame(\n",
    "\n",
    "                {\n",
    "\n",
    "                    \"Store\": g[\"Store\"].values,\n",
    "\n",
    "                    \"Date\": g[\"Date\"].values,\n",
    "\n",
    "                    \"y_pred\": yhat,\n",
    "\n",
    "                }\n",
    "\n",
    "            )\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    if not preds:\n",
    "\n",
    "        continue\n",
    "\n",
    "\n",
    "\n",
    "    pred_df = pd.concat(preds, ignore_index=True)\n",
    "\n",
    "    pred_df = pred_df.merge(\n",
    "\n",
    "        test[[\"Store\", \"Date\", \"Weekly_Sales\"]], on=[\"Store\", \"Date\"], how=\"left\"\n",
    "\n",
    "    ).rename(columns={\"Weekly_Sales\": \"y_true\"})\n",
    "\n",
    "\n",
    "\n",
    "    name = (\n",
    "\n",
    "        f\"deepar_exog__hs{cfg['hidden_size']}__nl{cfg['num_layers']}\"\n",
    "\n",
    "        f\"__do{cfg['dropout_rate']}__lr{cfg['lr']}__bs{cfg['batch_size']}\"\n",
    "\n",
    "    )\n",
    "\n",
    "    pred_df[\"model\"] = name\n",
    "\n",
    "    metrics = compute_metrics(pred_df[\"y_true\"].values, pred_df[\"y_pred\"].values)\n",
    "\n",
    "    results.append({\"cfg\": cfg, \"metrics\": metrics, \"name\": name})\n",
    "\n",
    "\n",
    "\n",
    "    save_outputs(\n",
    "\n",
    "        model_name=name,\n",
    "\n",
    "        predictions=pred_df,\n",
    "\n",
    "        metrics_global=pd.DataFrame([{**{\"model\": name}, **metrics}]),\n",
    "\n",
    "        metrics_by_store=pred_df.groupby(\"Store\").apply(\n",
    "\n",
    "            lambda g: pd.Series(compute_metrics(g[\"y_true\"].values, g[\"y_pred\"].values))\n",
    "\n",
    "        ).reset_index().assign(model=name),\n",
    "\n",
    "        output_dir=OUTPUTS_DIR,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame([\n",
    "\n",
    "    {\"model\": r[\"name\"], \"MAE\": r[\"metrics\"][\"MAE\"], \"RMSE\": r[\"metrics\"][\"RMSE\"], \"sMAPE\": r[\"metrics\"][\"sMAPE\"]}\n",
    "\n",
    "    for r in results\n",
    "\n",
    "])\n",
    "\n",
    "results_df = results_df.sort_values([\"sMAPE\", \"MAE\"]).reset_index(drop=True)\n",
    "\n",
    "best_model_name = results_df.loc[0, \"model\"]\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae28686",
   "metadata": {},
   "source": [
    "## 5) Métricas (MAE, RMSE, sMAPE)\n",
    "Se reporta:\n",
    "- Global\n",
    "- Por store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdffcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(         model           MAE         RMSE     sMAPE\n",
       " 0  deepar_exog  59573.684606  86287.61717  6.041327,\n",
       "          model  Store           MAE          RMSE     sMAPE\n",
       " 0  deepar_exog      1  63187.098125  72535.226686  4.063790\n",
       " 1  deepar_exog      2  57557.482500  71639.894989  3.080493\n",
       " 2  deepar_exog      3  10764.173750  15598.596845  2.592597\n",
       " 3  deepar_exog      4  39508.113750  50746.641501  1.861186\n",
       " 4  deepar_exog      5  14828.231719  17973.278986  4.584132)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resumen rápido: mejor config y (si existe) baseline de 20 epochs\n",
    "\n",
    "best_row = results_df.iloc[0]\n",
    "\n",
    "print(\"Mejor config (ordenada por sMAPE):\")\n",
    "\n",
    "print(best_row)\n",
    "\n",
    "\n",
    "\n",
    "import pathlib\n",
    "\n",
    "baseline_path = pathlib.Path(OUTPUTS_DIR) / \"metrics\" / \"deepar_exog_metrics_global.csv\"\n",
    "\n",
    "if baseline_path.exists():\n",
    "\n",
    "    baseline_df = pd.read_csv(baseline_path)\n",
    "\n",
    "    print(\"\\nBaseline (EPOCHS=20) encontrado:\")\n",
    "\n",
    "    print(baseline_df)\n",
    "\n",
    "    print(\"Comparación sMAPE delta:\", float(best_row[\"sMAPE\"]) - float(baseline_df.loc[0, \"sMAPE\"]))\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"\\nBaseline (EPOCHS=20) no encontrado; ejecuta el baseline para comparar.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d318126",
   "metadata": {},
   "source": [
    "## 6) Guardado de outputs estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e2f61893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': 'c:\\\\Users\\\\usuario\\\\Documents\\\\Master AI\\\\TFM\\\\MEMORIA 2.0\\\\outputs\\\\predictions\\\\deepar_exog_predictions.csv',\n",
       " 'metrics_global': 'c:\\\\Users\\\\usuario\\\\Documents\\\\Master AI\\\\TFM\\\\MEMORIA 2.0\\\\outputs\\\\metrics\\\\deepar_exog_metrics_global.csv',\n",
       " 'metrics_by_store': 'c:\\\\Users\\\\usuario\\\\Documents\\\\Master AI\\\\TFM\\\\MEMORIA 2.0\\\\outputs\\\\metrics\\\\deepar_exog_metrics_by_store.csv'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = save_outputs(\n",
    "    model_name=MODEL_NAME,\n",
    "    predictions=pred_df,\n",
    "    metrics_global=metrics_global_df,\n",
    "    metrics_by_store=metrics_by_store_df,\n",
    "    output_dir=OUTPUTS_DIR,\n",
    ")\n",
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2838ecac",
   "metadata": {},
   "source": [
    "## 7) Figuras\n",
    "- 3 tiendas: real vs predicción en test\n",
    "- Distribución del error (`y_true - y_pred`)\n",
    "\n",
    "Guardar PNGs en `outputs/figures/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8967b1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "FIG_DIR = OUTPUTS_DIR / \"figures\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Selección de 3 tiendas (mayor media de ventas en test)\n",
    "top_stores = (\n",
    "    pred_df.groupby(\"Store\")[\"y_true\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(3)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "for store in top_stores:\n",
    "    g = pred_df[pred_df[\"Store\"] == store].sort_values(\"Date\")\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(g[\"Date\"], g[\"y_true\"], label=\"y_true\")\n",
    "    plt.plot(g[\"Date\"], g[\"y_pred\"], label=\"y_pred\")\n",
    "    plt.title(f\"Store {store} — DeepAR\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Weekly_Sales\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{MODEL_NAME}_plot_store_{store}.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# Distribución de error\n",
    "errors = pred_df[\"y_true\"] - pred_df[\"y_pred\"]\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(errors, bins=30, kde=True)\n",
    "plt.title(\"Error distribution (y_true - y_pred)\")\n",
    "plt.xlabel(\"Error\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / f\"{MODEL_NAME}_plot_error_dist.png\", dpi=150)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
