{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ddbc4fc",
   "metadata": {},
   "source": [
    "# 05 — Transformer global con exógenas\n",
    "\n",
    "**Objetivo:** forecasting de `Weekly_Sales` semanal por `Store` usando Transformer global con covariables exógenas.\n",
    "\n",
    "## Supuesto experimental (oracle exog)\n",
    "Se asume disponibilidad de todas las covariables exógenas durante el horizonte de predicción (escenario oracle).\n",
    "\n",
    "## Outputs estándar\n",
    "- `outputs/predictions/transformer_exog_predictions.csv` con: `Store, Date, y_true, y_pred, model`\n",
    "- `outputs/metrics/transformer_exog_metrics_global.csv`\n",
    "- `outputs/metrics/transformer_exog_metrics_by_store.csv`\n",
    "- `outputs/figures/transformer_exog_plot_*.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_state": "idle",
   "id": "ea2d0035",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:33:38.462944Z",
     "iopub.status.busy": "2026-01-30T23:33:38.462484Z",
     "iopub.status.idle": "2026-01-30T23:33:38.822652Z",
     "shell.execute_reply": "2026-01-30T23:33:38.822081Z",
     "shell.execute_reply.started": "2026-01-30T23:33:38.462908Z"
    }
   },
   "outputs": [],
   "source": [
    "# 0) Imports y configuración\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.common import (\n",
    "    compute_metrics,\n",
    "    load_data,\n",
    "    make_features,\n",
    "    save_outputs,\n",
    "    temporal_split,\n",
    ")\n",
    "\n",
    "MODEL_NAME = 'transformer_exog'\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = PROJECT_ROOT / 'data' / 'Walmart_Sales.csv'\n",
    "METADATA_PATH = PROJECT_ROOT / 'outputs' / 'metadata.json'\n",
    "OUTPUTS_DIR = PROJECT_ROOT / 'outputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48241d46",
   "metadata": {},
   "source": [
    "## 1) Cargar metadata (split + features)\n",
    "Esto garantiza consistencia entre modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "id": "2d83cde3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:33:38.824227Z",
     "iopub.status.busy": "2026-01-30T23:33:38.823792Z",
     "iopub.status.idle": "2026-01-30T23:33:38.874767Z",
     "shell.execute_reply": "2026-01-30T23:33:38.874251Z",
     "shell.execute_reply.started": "2026-01-30T23:33:38.824192Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: {'train_start': '2010-02-05', 'train_end': '2012-07-06', 'val_start': '2012-07-13', 'val_end': '2012-08-31', 'test_start': '2012-09-07', 'test_end': '2012-10-26'}\n",
      "N features: 16\n"
     ]
    }
   ],
   "source": [
    "metadata = json.loads(METADATA_PATH.read_text(encoding='utf-8'))\n",
    "split = metadata['split']\n",
    "feature_cols = metadata['features']\n",
    "print('Split:', split)\n",
    "print('N features:', len(feature_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd048bc",
   "metadata": {},
   "source": [
    "## 2) Carga de datos + features\n",
    "- Parseo/orden\n",
    "- Construcción de lags/rolling (sin leakage)\n",
    "- Exógenas alineadas por fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "execution_state": "idle",
   "id": "d9f97a9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:33:38.875943Z",
     "iopub.status.busy": "2026-01-30T23:33:38.875561Z",
     "iopub.status.idle": "2026-01-30T23:33:38.967108Z",
     "shell.execute_reply": "2026-01-30T23:33:38.966550Z",
     "shell.execute_reply.started": "2026-01-30T23:33:38.875923Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4095, 22)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data(DATA_PATH)\n",
    "df_feat, _ = make_features(df, add_calendar=True)\n",
    "\n",
    "# Importante: para entrenar, debes decidir cómo tratar NaNs creados por lags/rolling\n",
    "# Opción típica: descartar filas con NaNs en features (por store al inicio)\n",
    "model_df = df_feat.dropna(subset=feature_cols + ['Weekly_Sales']).copy()\n",
    "model_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a122d5d6",
   "metadata": {},
   "source": [
    "## 3) Split temporal\n",
    "Reutiliza exactamente el split definido en el notebook 00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "execution_state": "idle",
   "id": "badc5119",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:33:38.969159Z",
     "iopub.status.busy": "2026-01-30T23:33:38.968913Z",
     "iopub.status.idle": "2026-01-30T23:33:38.986637Z",
     "shell.execute_reply": "2026-01-30T23:33:38.986145Z",
     "shell.execute_reply.started": "2026-01-30T23:33:38.969140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3375 360 360\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df, split_cfg = temporal_split(df)\n",
    "\n",
    "# Aplicar el split sobre model_df (ya sin NaNs por lags)\n",
    "train = model_df[model_df['Date'].between(split_cfg.train_start, split_cfg.train_end)].copy()\n",
    "val = model_df[model_df['Date'].between(split_cfg.val_start, split_cfg.val_end)].copy()\n",
    "test = model_df[model_df['Date'].between(split_cfg.test_start, split_cfg.test_end)].copy()\n",
    "\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2403b7a",
   "metadata": {},
   "source": [
    "## 4) Entrenamiento del modelo\n",
    "Implementación Transformer global con covariables exógenas.\n",
    "Incluye representaciones de Store (one-hot o embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "execution_state": "idle",
   "id": "7a76f46d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:33:38.987464Z",
     "iopub.status.busy": "2026-01-30T23:33:38.987291Z",
     "iopub.status.idle": "2026-01-30T23:33:38.990576Z",
     "shell.execute_reply": "2026-01-30T23:33:38.990080Z",
     "shell.execute_reply.started": "2026-01-30T23:33:38.987448Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: implementar entrenamiento Transformer con exógenas\n",
    "# Debe producir predicciones para TEST (ideal: también para VAL).\n",
    "y_pred_test = np.full(shape=len(test), fill_value=test['Weekly_Sales'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48213bf3",
   "metadata": {},
   "source": [
    "## 5) Métricas (MAE, RMSE, sMAPE)\n",
    "Se reporta:\n",
    "- Global\n",
    "- Por store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "execution_state": "idle",
   "id": "026c22ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:33:38.991541Z",
     "iopub.status.busy": "2026-01-30T23:33:38.991362Z",
     "iopub.status.idle": "2026-01-30T23:34:14.088266Z",
     "shell.execute_reply": "2026-01-30T23:34:14.087727Z",
     "shell.execute_reply.started": "2026-01-30T23:33:38.991524Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Transformer config {'d_model': 64, 'nhead': 4, 'num_layers': 2, 'dropout': 0.1, 'lr': 0.001} ===\n",
      "Epoch 5/200 | train=0.0309 | val=0.0438 | lr=1.00e-03\n",
      "Epoch 10/200 | train=0.0227 | val=0.0809 | lr=1.00e-03\n",
      "Epoch 15/200 | train=0.0127 | val=0.0448 | lr=5.00e-04\n",
      "Epoch 20/200 | train=0.0088 | val=0.0500 | lr=2.50e-04\n",
      "Early stopping at epoch 21 (best val 0.0220)\n",
      "\n",
      "=== Transformer config {'d_model': 64, 'nhead': 4, 'num_layers': 2, 'dropout': 0.2, 'lr': 0.0003} ===\n",
      "Epoch 5/200 | train=0.0326 | val=0.0419 | lr=3.00e-04\n",
      "Epoch 10/200 | train=0.0287 | val=0.0319 | lr=1.50e-04\n",
      "Epoch 15/200 | train=0.0223 | val=0.0277 | lr=1.50e-04\n",
      "Early stopping at epoch 19 (best val 0.0214)\n",
      "\n",
      "=== Transformer config {'d_model': 128, 'nhead': 8, 'num_layers': 4, 'dropout': 0.1, 'lr': 0.0003} ===\n",
      "Epoch 5/200 | train=0.0555 | val=0.0201 | lr=3.00e-04\n",
      "Epoch 10/200 | train=0.0350 | val=0.0233 | lr=3.00e-04\n",
      "Epoch 15/200 | train=0.0152 | val=0.0173 | lr=1.50e-04\n",
      "Epoch 20/200 | train=0.0126 | val=0.0190 | lr=1.50e-04\n",
      "Epoch 25/200 | train=0.0109 | val=0.0173 | lr=7.50e-05\n",
      "Epoch 30/200 | train=0.0093 | val=0.0158 | lr=3.75e-05\n",
      "Early stopping at epoch 33 (best val 0.0153)\n",
      "\n",
      "=== Transformer config {'d_model': 128, 'nhead': 8, 'num_layers': 2, 'dropout': 0.2, 'lr': 0.001} ===\n",
      "Epoch 5/200 | train=0.0463 | val=0.0302 | lr=1.00e-03\n",
      "Epoch 10/200 | train=0.0256 | val=0.0523 | lr=1.00e-03\n",
      "Epoch 15/200 | train=0.0176 | val=0.0528 | lr=5.00e-04\n",
      "Epoch 20/200 | train=0.0112 | val=0.0411 | lr=2.50e-04\n",
      "Early stopping at epoch 20 (best val 0.0302)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>sMAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transformer_exog__dm128__nh8__nl4__do0.1__lr0....</td>\n",
       "      <td>50989.601924</td>\n",
       "      <td>68451.731397</td>\n",
       "      <td>5.528291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transformer_exog__dm64__nh4__nl2__do0.2__lr0.0003</td>\n",
       "      <td>64173.725741</td>\n",
       "      <td>85342.678812</td>\n",
       "      <td>6.693672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transformer_exog__dm64__nh4__nl2__do0.1__lr0.001</td>\n",
       "      <td>87233.718505</td>\n",
       "      <td>105921.993456</td>\n",
       "      <td>10.930270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transformer_exog__dm128__nh8__nl2__do0.2__lr0.001</td>\n",
       "      <td>82876.353522</td>\n",
       "      <td>104778.980245</td>\n",
       "      <td>11.054150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model           MAE  \\\n",
       "0  transformer_exog__dm128__nh8__nl4__do0.1__lr0....  50989.601924   \n",
       "1  transformer_exog__dm64__nh4__nl2__do0.2__lr0.0003  64173.725741   \n",
       "2   transformer_exog__dm64__nh4__nl2__do0.1__lr0.001  87233.718505   \n",
       "3  transformer_exog__dm128__nh8__nl2__do0.2__lr0.001  82876.353522   \n",
       "\n",
       "            RMSE      sMAPE  \n",
       "0   68451.731397   5.528291  \n",
       "1   85342.678812   6.693672  \n",
       "2  105921.993456  10.930270  \n",
       "3  104778.980245  11.054150  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from warnings import filterwarnings\n",
    "\n",
    "\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    import torch\n",
    "\n",
    "    from torch import nn\n",
    "\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "except Exception as exc:\n",
    "\n",
    "    raise ImportError(\n",
    "\n",
    "        \"PyTorch no está instalado. Instala con: pip install torch\"\n",
    "\n",
    "    ) from exc\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "# Helper para features autoregresivas (sin leakage)\n",
    "\n",
    "def _compute_feature_vector(y_hist, date, exog_row, cfg):\n",
    "\n",
    "    lags = cfg[\"lags\"]\n",
    "\n",
    "    rollings = cfg[\"rollings\"]\n",
    "\n",
    "    add_calendar = bool(cfg.get(\"add_calendar\", True))\n",
    "\n",
    "    exog_cols = cfg[\"exog_cols\"]\n",
    "\n",
    "    feat = {}\n",
    "\n",
    "    for k in lags:\n",
    "\n",
    "        feat[f\"lag_{k}\"] = y_hist[-k] if len(y_hist) >= k else np.nan\n",
    "\n",
    "    for w in rollings:\n",
    "\n",
    "        if len(y_hist) >= w:\n",
    "\n",
    "            window = np.array(y_hist[-w:], dtype=float)\n",
    "\n",
    "            feat[f\"roll_mean_{w}\"] = float(window.mean())\n",
    "\n",
    "            feat[f\"roll_std_{w}\"] = float(window.std(ddof=0))\n",
    "\n",
    "        else:\n",
    "\n",
    "            feat[f\"roll_mean_{w}\"] = np.nan\n",
    "\n",
    "            feat[f\"roll_std_{w}\"] = np.nan\n",
    "\n",
    "    for c in exog_cols:\n",
    "\n",
    "        feat[c] = float(exog_row[c])\n",
    "\n",
    "    if add_calendar:\n",
    "        iso = pd.Timestamp(date).isocalendar()\n",
    "        week_val = iso.week if hasattr(iso, 'week') else iso[1]\n",
    "        year_val = iso.year if hasattr(iso, 'year') else iso[0]\n",
    "        feat[\"weekofyear\"] = int(week_val)\n",
    "        feat[\"month\"] = int(pd.Timestamp(date).month)\n",
    "        feat[\"year\"] = int(year_val)\n",
    "\n",
    "    vec = [feat.get(c, np.nan) for c in feature_cols]\n",
    "\n",
    "    return vec\n",
    "\n",
    "\n",
    "\n",
    "# Configuración base + search space\n",
    "\n",
    "EPOCHS_MAX = 200\n",
    "\n",
    "PATIENCE_ES = 15\n",
    "\n",
    "MIN_DELTA = 1e-4\n",
    "\n",
    "CLIP_NORM = 1.0\n",
    "\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "REDUCE_LR_FACTOR = 0.5\n",
    "\n",
    "REDUCE_LR_PATIENCE = 5\n",
    "\n",
    "EMB_DIM = 16\n",
    "\n",
    "WINDOW = 52\n",
    "\n",
    "\n",
    "\n",
    "transformer_search = [\n",
    "\n",
    "    {\"d_model\": 64, \"nhead\": 4, \"num_layers\": 2, \"dropout\": 0.1, \"lr\": 1e-3},\n",
    "\n",
    "    {\"d_model\": 64, \"nhead\": 4, \"num_layers\": 2, \"dropout\": 0.2, \"lr\": 3e-4},\n",
    "\n",
    "    {\"d_model\": 128, \"nhead\": 8, \"num_layers\": 4, \"dropout\": 0.1, \"lr\": 3e-4},\n",
    "\n",
    "    {\"d_model\": 128, \"nhead\": 8, \"num_layers\": 2, \"dropout\": 0.2, \"lr\": 1e-3},\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Anti-leakage\n",
    "\n",
    "assert split_cfg.train_end < split_cfg.val_start < split_cfg.val_end < split_cfg.test_start <= split_cfg.test_end\n",
    "\n",
    "\n",
    "\n",
    "# Mapping Store -> idx para embeddings\n",
    "\n",
    "stores_sorted = sorted(model_df[\"Store\"].unique())\n",
    "\n",
    "store_to_idx = {s: i for i, s in enumerate(stores_sorted)}\n",
    "\n",
    "num_stores = len(store_to_idx)\n",
    "\n",
    "\n",
    "\n",
    "feature_cols_model = feature_cols\n",
    "DEFAULT_LAGS = sorted({int(c.split(\"_\")[1]) for c in feature_cols if c.startswith(\"lag_\")})\n",
    "DEFAULT_ROLLINGS = sorted({int(c.split(\"_\")[2]) for c in feature_cols if c.startswith(\"roll_mean_\")})\n",
    "EXOG_COLUMNS = [c for c in feature_cols if not c.startswith(\"lag_\") and not c.startswith(\"roll_\")]\n",
    "\n",
    "\n",
    "\n",
    "# Escaladores SOLO con train\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "train_fit = train.copy()\n",
    "\n",
    "scaler_x.fit(train_fit[feature_cols_model].values)\n",
    "\n",
    "scaler_y.fit(train_fit[[\"Weekly_Sales\"]].values)\n",
    "\n",
    "assert hasattr(scaler_x, \"mean_\") and hasattr(scaler_y, \"mean_\")\n",
    "\n",
    "\n",
    "\n",
    "def build_sequences(df_in: pd.DataFrame, window: int = WINDOW):\n",
    "\n",
    "    sequences, targets, dates, stores_idx = [], [], [], []\n",
    "\n",
    "    df_in = df_in.sort_values([\"Store\", \"Date\"]).copy()\n",
    "\n",
    "    for store, g in df_in.groupby(\"Store\"):\n",
    "\n",
    "        g = g.sort_values(\"Date\")\n",
    "\n",
    "        X = scaler_x.transform(g[feature_cols_model].values)\n",
    "\n",
    "        y = scaler_y.transform(g[[\"Weekly_Sales\"]].values).ravel()\n",
    "\n",
    "        date_arr = g[\"Date\"].values\n",
    "\n",
    "        s_idx = store_to_idx[int(store)]\n",
    "\n",
    "        for t in range(window, len(g)):\n",
    "\n",
    "            sequences.append(X[t - window : t])\n",
    "\n",
    "            targets.append(y[t])\n",
    "\n",
    "            dates.append(date_arr[t])\n",
    "\n",
    "            stores_idx.append(s_idx)\n",
    "\n",
    "    return (\n",
    "\n",
    "        np.array(sequences),\n",
    "\n",
    "        np.array(targets),\n",
    "\n",
    "        np.array(dates),\n",
    "\n",
    "        np.array(stores_idx, dtype=int),\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "X_seq, y_seq, d_seq, s_seq = build_sequences(model_df, window=WINDOW)\n",
    "\n",
    "\n",
    "\n",
    "# Split por fecha (sin leakage)\n",
    "\n",
    "train_mask = (d_seq >= split_cfg.train_start) & (d_seq <= split_cfg.train_end)\n",
    "\n",
    "val_mask = (d_seq >= split_cfg.val_start) & (d_seq <= split_cfg.val_end)\n",
    "\n",
    "test_mask = (d_seq >= split_cfg.test_start) & (d_seq <= split_cfg.test_end)\n",
    "\n",
    "assert not (train_mask & val_mask).any() and not (train_mask & test_mask).any() and not (val_mask & test_mask).any()\n",
    "\n",
    "\n",
    "\n",
    "X_train, y_train, s_train = X_seq[train_mask], y_seq[train_mask], s_seq[train_mask]\n",
    "\n",
    "X_val, y_val, s_val = X_seq[val_mask], y_seq[val_mask], s_seq[val_mask]\n",
    "\n",
    "X_test_seq, y_test_seq, s_test_seq = X_seq[test_mask], y_seq[test_mask], s_seq[test_mask]\n",
    "\n",
    "test_dates = d_seq[test_mask]\n",
    "\n",
    "test_stores = s_seq[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "\n",
    "    TensorDataset(\n",
    "\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "\n",
    "        torch.tensor(s_train, dtype=torch.long),\n",
    "\n",
    "        torch.tensor(y_train, dtype=torch.float32),\n",
    "\n",
    "    ),\n",
    "\n",
    "    batch_size=64,\n",
    "\n",
    "    shuffle=True,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "val_loader = DataLoader(\n",
    "\n",
    "    TensorDataset(\n",
    "\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "\n",
    "        torch.tensor(s_val, dtype=torch.long),\n",
    "\n",
    "        torch.tensor(y_val, dtype=torch.float32),\n",
    "\n",
    "    ),\n",
    "\n",
    "    batch_size=64,\n",
    "\n",
    "    shuffle=False,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "class TransformerRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, d_model: int, nhead: int, num_layers: int, dropout: float, max_len: int, emb_dim: int, num_stores: int):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_proj = nn.Linear(input_size, d_model)\n",
    "\n",
    "        self.store_emb = nn.Embedding(num_stores, emb_dim)\n",
    "\n",
    "        self.store_proj = nn.Linear(emb_dim, d_model)\n",
    "\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "        self.max_len = max_len\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, store_idx):\n",
    "\n",
    "        b, t, _ = x.size()\n",
    "\n",
    "        pos_idx = torch.arange(t, device=x.device)\n",
    "\n",
    "        pos = self.pos_emb(pos_idx).unsqueeze(0).expand(b, t, -1)\n",
    "\n",
    "        emb = self.store_proj(self.store_emb(store_idx)).unsqueeze(1).expand(b, t, -1)\n",
    "\n",
    "        h = self.input_proj(x) + emb + pos\n",
    "\n",
    "        h = self.encoder(h)\n",
    "\n",
    "        out = h[:, -1, :]\n",
    "\n",
    "        return self.fc(out).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "def train_eval(config_run: dict):\n",
    "\n",
    "    model = TransformerRegressor(\n",
    "\n",
    "        input_size=X_train.shape[-1],\n",
    "\n",
    "        d_model=config_run[\"d_model\"],\n",
    "\n",
    "        nhead=config_run[\"nhead\"],\n",
    "\n",
    "        num_layers=config_run[\"num_layers\"],\n",
    "\n",
    "        dropout=config_run[\"dropout\"],\n",
    "\n",
    "        max_len=WINDOW,\n",
    "\n",
    "        emb_dim=EMB_DIM,\n",
    "\n",
    "        num_stores=num_stores,\n",
    "\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "\n",
    "        model.parameters(), lr=config_run[\"lr\"], weight_decay=WEIGHT_DECAY\n",
    "\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "\n",
    "        optimizer, mode=\"min\", factor=REDUCE_LR_FACTOR, patience=REDUCE_LR_PATIENCE, verbose=False\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "\n",
    "    best_state = None\n",
    "\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    history = []\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCHS_MAX):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        train_losses = []\n",
    "\n",
    "        for xb, sb, yb in train_loader:\n",
    "\n",
    "            xb, sb, yb = xb.to(device), sb.to(device), yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = model(xb, sb)\n",
    "\n",
    "            loss = criterion(preds, yb)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        val_losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for xb, sb, yb in val_loader:\n",
    "\n",
    "                xb, sb, yb = xb.to(device), sb.to(device), yb.to(device)\n",
    "\n",
    "                preds = model(xb, sb)\n",
    "\n",
    "                val_losses.append(criterion(preds, yb).item())\n",
    "\n",
    "\n",
    "\n",
    "        train_loss = float(np.mean(train_losses))\n",
    "\n",
    "        val_loss = float(np.mean(val_losses)) if val_losses else train_loss\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        history.append((epoch + 1, train_loss, val_loss, current_lr))\n",
    "\n",
    "\n",
    "\n",
    "        if val_loss + MIN_DELTA < best_val:\n",
    "\n",
    "            best_val = val_loss\n",
    "\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "        else:\n",
    "\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS_MAX} | train={train_loss:.4f} | val={val_loss:.4f} | lr={current_lr:.2e}\")\n",
    "\n",
    "\n",
    "\n",
    "        if epochs_no_improve >= PATIENCE_ES:\n",
    "\n",
    "            print(f\"Early stopping at epoch {epoch+1} (best val {best_val:.4f})\")\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    if best_state is not None:\n",
    "\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "\n",
    "\n",
    "\n",
    "    return model, best_val, history\n",
    "\n",
    "\n",
    "\n",
    "def predict_test(model):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for store, g_future in test.groupby(\"Store\"):\n",
    "\n",
    "        g_future = g_future.sort_values(\"Date\")\n",
    "\n",
    "        g_hist = train[train[\"Store\"] == store].sort_values(\"Date\")\n",
    "\n",
    "        if g_hist.empty:\n",
    "\n",
    "            continue\n",
    "\n",
    "        y_hist = g_hist[\"Weekly_Sales\"].tolist()\n",
    "\n",
    "        feat_hist = []\n",
    "\n",
    "        for _, row in g_hist.iterrows():\n",
    "\n",
    "            feat_hist.append(\n",
    "\n",
    "                _compute_feature_vector(\n",
    "\n",
    "                    y_hist[: g_hist.index.get_loc(row.name) + 1],\n",
    "\n",
    "                    row[\"Date\"],\n",
    "\n",
    "                    row,\n",
    "\n",
    "                    {'lags': DEFAULT_LAGS, 'rollings': DEFAULT_ROLLINGS, 'add_calendar': True, 'exog_cols': EXOG_COLUMNS, 'feature_cols': feature_cols},\n",
    "\n",
    "                )\n",
    "\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        store_preds = []\n",
    "\n",
    "        s_idx = store_to_idx[int(store)]\n",
    "\n",
    "        for _, row in g_future.iterrows():\n",
    "\n",
    "            feat_vec = _compute_feature_vector(\n",
    "\n",
    "                y_hist,\n",
    "\n",
    "                row[\"Date\"],\n",
    "\n",
    "                row,\n",
    "\n",
    "                {'lags': DEFAULT_LAGS, 'rollings': DEFAULT_ROLLINGS, 'add_calendar': True, 'exog_cols': EXOG_COLUMNS, 'feature_cols': feature_cols},\n",
    "\n",
    "            )\n",
    "\n",
    "            feat_hist.append(feat_vec)\n",
    "\n",
    "            seq = np.array(feat_hist[-WINDOW :], dtype=float)\n",
    "\n",
    "            seq = np.nan_to_num(seq, nan=train[\"Weekly_Sales\"].mean())\n",
    "\n",
    "            seq_scaled = scaler_x.transform(seq)\n",
    "\n",
    "            xb = torch.tensor(seq_scaled, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            sb = torch.tensor([s_idx], dtype=torch.long).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                yhat_scaled = model(xb, sb).cpu().numpy().ravel()[0]\n",
    "\n",
    "            yhat = scaler_y.inverse_transform([[yhat_scaled]])[0][0]\n",
    "\n",
    "            y_hist.append(float(yhat))\n",
    "\n",
    "            store_preds.append(float(yhat))\n",
    "\n",
    "\n",
    "\n",
    "        preds.append(\n",
    "\n",
    "            pd.DataFrame(\n",
    "\n",
    "                {\n",
    "\n",
    "                    \"Store\": g_future[\"Store\"].values,\n",
    "\n",
    "                    \"Date\": g_future[\"Date\"].values,\n",
    "\n",
    "                    \"y_pred\": store_preds,\n",
    "\n",
    "                }\n",
    "\n",
    "            )\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    if not preds:\n",
    "\n",
    "        return pd.DataFrame(columns=[\"Store\", \"Date\", \"y_pred\"])\n",
    "\n",
    "    return pd.concat(preds, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for cfg in transformer_search:\n",
    "\n",
    "    print(f\"\\n=== Transformer config {cfg} ===\")\n",
    "\n",
    "    model, best_val, history = train_eval(cfg)\n",
    "\n",
    "    pred_df = predict_test(model)\n",
    "\n",
    "    pred_df = pred_df.merge(\n",
    "\n",
    "        test[[\"Store\", \"Date\", \"Weekly_Sales\"]], on=[\"Store\", \"Date\"], how=\"left\"\n",
    "\n",
    "    ).rename(columns={\"Weekly_Sales\": \"y_true\"})\n",
    "\n",
    "    name = f\"transformer_exog__dm{cfg['d_model']}__nh{cfg['nhead']}__nl{cfg['num_layers']}__do{cfg['dropout']}__lr{cfg['lr']}\"\n",
    "\n",
    "    pred_df[\"model\"] = name\n",
    "\n",
    "    metrics = compute_metrics(pred_df[\"y_true\"].values, pred_df[\"y_pred\"].values)\n",
    "\n",
    "    results.append({\"cfg\": cfg, \"metrics\": metrics, \"name\": name})\n",
    "\n",
    "    save_outputs(\n",
    "\n",
    "        model_name=name,\n",
    "\n",
    "        predictions=pred_df,\n",
    "\n",
    "        metrics_global=pd.DataFrame([{**{\"model\": name}, **metrics}]),\n",
    "\n",
    "        metrics_by_store=pred_df.groupby(\"Store\").apply(\n",
    "\n",
    "            lambda g: pd.Series(compute_metrics(g[\"y_true\"].values, g[\"y_pred\"].values))\n",
    "\n",
    "        ).reset_index().assign(model=name),\n",
    "\n",
    "        output_dir=OUTPUTS_DIR,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame([\n",
    "\n",
    "    {\"model\": r[\"name\"], \"MAE\": r[\"metrics\"][\"MAE\"], \"RMSE\": r[\"metrics\"][\"RMSE\"], \"sMAPE\": r[\"metrics\"][\"sMAPE\"]}\n",
    "\n",
    "    for r in results\n",
    "\n",
    "])\n",
    "\n",
    "results_df = results_df.sort_values([\"sMAPE\", \"MAE\"]).reset_index(drop=True)\n",
    "\n",
    "best_model_name = results_df.loc[0, \"model\"]\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91980d50",
   "metadata": {},
   "source": [
    "## 5) Métricas (MAE, RMSE, sMAPE)\n",
    "Se reporta:\n",
    "- Global\n",
    "- Por store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "execution_state": "idle",
   "id": "a91f5e1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:34:14.089179Z",
     "iopub.status.busy": "2026-01-30T23:34:14.088928Z",
     "iopub.status.idle": "2026-01-30T23:34:26.461604Z",
     "shell.execute_reply": "2026-01-30T23:34:26.460931Z",
     "shell.execute_reply.started": "2026-01-30T23:34:14.089161Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-entrenando mejor config para outputs: {'d_model': 128, 'nhead': 8, 'num_layers': 4, 'dropout': 0.1, 'lr': 0.0003}\n",
      "Epoch 5/200 | train=0.0422 | val=0.0450 | lr=3.00e-04\n",
      "Epoch 10/200 | train=0.0379 | val=0.0445 | lr=1.50e-04\n",
      "Epoch 15/200 | train=0.0163 | val=0.0392 | lr=1.50e-04\n",
      "Epoch 20/200 | train=0.0126 | val=0.0335 | lr=7.50e-05\n",
      "Epoch 25/200 | train=0.0107 | val=0.0269 | lr=3.75e-05\n",
      "Early stopping at epoch 26 (best val 0.0167)\n"
     ]
    }
   ],
   "source": [
    "# Re-entrenar mejor config para guardar outputs consistentes\n",
    "best_cfg = next(r[\"cfg\"] for r in results if r[\"name\"] == best_model_name)\n",
    "print(\"Re-entrenando mejor config para outputs:\", best_cfg)\n",
    "\n",
    "best_model, best_val, _ = train_eval(best_cfg)\n",
    "\n",
    "pred_df = predict_test(best_model)\n",
    "pred_df = pred_df.merge(\n",
    "    test[[\"Store\", \"Date\", \"Weekly_Sales\"]], on=[\"Store\", \"Date\"], how=\"left\"\n",
    ").rename(columns={\"Weekly_Sales\": \"y_true\"})\n",
    "pred_df[\"model\"] = best_model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "execution_state": "idle",
   "id": "3f51d4b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:34:26.462644Z",
     "iopub.status.busy": "2026-01-30T23:34:26.462338Z",
     "iopub.status.idle": "2026-01-30T23:34:26.608639Z",
     "shell.execute_reply": "2026-01-30T23:34:26.608023Z",
     "shell.execute_reply.started": "2026-01-30T23:34:26.462622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor config (ordenada por sMAPE):\n",
      "model    transformer_exog__dm128__nh8__nl4__do0.1__lr0....\n",
      "MAE                                           50989.601924\n",
      "RMSE                                          68451.731397\n",
      "sMAPE                                             5.528291\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Baseline (EPOCHS=20) encontrado:\n",
      "                                               model            MAE  \\\n",
      "0  transformer_exog__dm64__nh4__nl2__do0.2__lr0.0003  131750.126301   \n",
      "\n",
      "            RMSE      sMAPE  \n",
      "0  157323.522886  12.253173  \n",
      "Comparación sMAPE delta: -6.7248821273217025\n"
     ]
    }
   ],
   "source": [
    "# Resumen rápido: mejor config y (si existe) baseline de 20 epochs\n",
    "if \"metrics_global_df\" in globals():\n",
    "    best_row = metrics_global_df.iloc[0]\n",
    "elif \"results_df\" in globals():\n",
    "    best_row = results_df.iloc[0]\n",
    "else:\n",
    "    raise ValueError(\"No hay resultados para mostrar.\")\n",
    "\n",
    "print(\"Mejor config (ordenada por sMAPE):\")\n",
    "print(best_row)\n",
    "\n",
    "# Intentar cargar baseline (si ya existe en outputs)\n",
    "if \"baseline_path\" not in globals():\n",
    "    import pathlib\n",
    "    baseline_path = pathlib.Path(OUTPUTS_DIR) / \"metrics\" / \"transformer_exog_metrics_global.csv\"\n",
    "\n",
    "if baseline_path.exists():\n",
    "    baseline_df = pd.read_csv(baseline_path)\n",
    "    print(\"\\nBaseline (EPOCHS=20) encontrado:\")\n",
    "    print(baseline_df)\n",
    "    print(\"Comparación sMAPE delta:\", float(best_row[\"sMAPE\"]) - float(baseline_df.loc[0, \"sMAPE\"]))\n",
    "else:\n",
    "    print(\"\\nBaseline (EPOCHS=20) no encontrado; ejecuta el baseline para comparar.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6705a3",
   "metadata": {},
   "source": [
    "## 6) Guardado de outputs estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "execution_state": "idle",
   "id": "8717a65c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:34:26.609549Z",
     "iopub.status.busy": "2026-01-30T23:34:26.609302Z",
     "iopub.status.idle": "2026-01-30T23:34:26.640601Z",
     "shell.execute_reply": "2026-01-30T23:34:26.640031Z",
     "shell.execute_reply.started": "2026-01-30T23:34:26.609530Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                               model           MAE  \\\n",
       " 0  transformer_exog__dm128__nh8__nl4__do0.1__lr0....  62559.516655   \n",
       " \n",
       "            RMSE     sMAPE  \n",
       " 0  80319.226581  7.016417  ,\n",
       "                                                model  Store            MAE  \\\n",
       " 0  transformer_exog__dm128__nh8__nl4__do0.1__lr0....      1  102878.911490   \n",
       " 1  transformer_exog__dm128__nh8__nl4__do0.1__lr0....      2   59977.583441   \n",
       " 2  transformer_exog__dm128__nh8__nl4__do0.1__lr0....      3   48585.362672   \n",
       " 3  transformer_exog__dm128__nh8__nl4__do0.1__lr0....      4   56085.615658   \n",
       " 4  transformer_exog__dm128__nh8__nl4__do0.1__lr0....      5   63945.339669   \n",
       " \n",
       "             RMSE      sMAPE  \n",
       " 0  116253.903474   6.543302  \n",
       " 1   72372.607645   3.204924  \n",
       " 2   52456.404451  11.133949  \n",
       " 3   67195.952295   2.624940  \n",
       " 4   64936.520738  18.198053  )"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Métricas globales y por tienda\n",
    "metrics_global = compute_metrics(pred_df[\"y_true\"].values, pred_df[\"y_pred\"].values)\n",
    "metrics_global_df = pd.DataFrame([\n",
    "    {\"model\": best_model_name, **metrics_global}\n",
    "])\n",
    "\n",
    "metrics_by_store_df = (\n",
    "    pred_df.groupby(\"Store\")\n",
    "    .apply(lambda g: pd.Series(compute_metrics(g[\"y_true\"].values, g[\"y_pred\"].values)))\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"MAE\", 1: \"RMSE\", 2: \"sMAPE\"})\n",
    ")\n",
    "metrics_by_store_df.insert(0, \"model\", best_model_name)\n",
    "metrics_by_store_df = metrics_by_store_df.sort_values(\"Store\")\n",
    "\n",
    "metrics_global_df, metrics_by_store_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "execution_state": "idle",
   "id": "cfc3a444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:34:26.642333Z",
     "iopub.status.busy": "2026-01-30T23:34:26.642038Z",
     "iopub.status.idle": "2026-01-30T23:34:26.895013Z",
     "shell.execute_reply": "2026-01-30T23:34:26.894412Z",
     "shell.execute_reply.started": "2026-01-30T23:34:26.642309Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': '/mnt/custom-file-systems/s3/shared/TFMAXEL/outputs/predictions/transformer_exog_predictions.csv',\n",
       " 'metrics_global': '/mnt/custom-file-systems/s3/shared/TFMAXEL/outputs/metrics/transformer_exog_metrics_global.csv',\n",
       " 'metrics_by_store': '/mnt/custom-file-systems/s3/shared/TFMAXEL/outputs/metrics/transformer_exog_metrics_by_store.csv'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = save_outputs(\n",
    "    model_name=MODEL_NAME,\n",
    "    predictions=pred_df,\n",
    "    metrics_global=metrics_global_df,\n",
    "    metrics_by_store=metrics_by_store_df,\n",
    "    output_dir=OUTPUTS_DIR,\n",
    ")\n",
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a98bd8",
   "metadata": {},
   "source": [
    "## 7) Figuras\n",
    "- 3 tiendas: real vs predicción en test\n",
    "- Distribución del error (`y_true - y_pred`)\n",
    "\n",
    "Guardar PNGs en `outputs/figures/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "execution_state": "idle",
   "id": "87d9b4e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-30T23:34:26.896208Z",
     "iopub.status.busy": "2026-01-30T23:34:26.895867Z",
     "iopub.status.idle": "2026-01-30T23:34:29.145204Z",
     "shell.execute_reply": "2026-01-30T23:34:29.144198Z",
     "shell.execute_reply.started": "2026-01-30T23:34:26.896180Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "FIG_DIR = OUTPUTS_DIR / \"figures\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Selección de 3 tiendas (mayor media de ventas en test)\n",
    "top_stores = (\n",
    "    pred_df.groupby(\"Store\")[\"y_true\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(3)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "for store in top_stores:\n",
    "    g = pred_df[pred_df[\"Store\"] == store].sort_values(\"Date\")\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(g[\"Date\"], g[\"y_true\"], label=\"y_true\")\n",
    "    plt.plot(g[\"Date\"], g[\"y_pred\"], label=\"y_pred\")\n",
    "    plt.title(f\"Store {store} — Transformer\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Weekly_Sales\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{MODEL_NAME}_plot_store_{store}.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# Distribución de error\n",
    "errors = pred_df[\"y_true\"] - pred_df[\"y_pred\"]\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(errors, bins=30, kde=True)\n",
    "plt.title(\"Error distribution (y_true - y_pred)\")\n",
    "plt.xlabel(\"Error\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / f\"{MODEL_NAME}_plot_error_dist.png\", dpi=150)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM (venv)",
   "language": "python",
   "name": "tfm-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
