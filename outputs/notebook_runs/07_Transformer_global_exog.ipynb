{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ddbc4fc",
   "metadata": {
    "papermill": {
     "duration": 0.006105,
     "end_time": "2026-02-02T11:42:30.171622",
     "exception": false,
     "start_time": "2026-02-02T11:42:30.165517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 05 — Transformer global con exógenas\n",
    "\n",
    "**Objetivo:** forecasting de `Weekly_Sales` semanal por `Store` usando Transformer global con covariables exógenas.\n",
    "\n",
    "## Supuesto experimental (oracle exog)\n",
    "Se asume disponibilidad de todas las covariables exógenas durante el horizonte de predicción (escenario oracle).\n",
    "\n",
    "## Outputs estándar\n",
    "- `outputs/predictions/transformer_exog_predictions.csv` con: `Store, Date, y_true, y_pred, model`\n",
    "- `outputs/metrics/transformer_exog_metrics_global.csv`\n",
    "- `outputs/metrics/transformer_exog_metrics_by_store.csv`\n",
    "- `outputs/figures/transformer_exog_plot_*.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "execution_state": "idle",
   "id": "ea2d0035",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:42:30.182124Z",
     "iopub.status.busy": "2026-02-02T11:42:30.181777Z",
     "iopub.status.idle": "2026-02-02T11:42:30.476635Z",
     "shell.execute_reply": "2026-02-02T11:42:30.476077Z"
    },
    "papermill": {
     "duration": 0.301463,
     "end_time": "2026-02-02T11:42:30.477821",
     "exception": false,
     "start_time": "2026-02-02T11:42:30.176358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 0) Imports y configuración\n",
    "from __future__ import annotations\n",
    "\n",
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "from src.common import (\n",
    "    compute_metrics,\n",
    "    load_data,\n",
    "    make_features,\n",
    "    save_outputs,\n",
    "    temporal_split,\n",
    ")\n",
    "\n",
    "MODEL_NAME = 'transformer_exog'\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_PATH = PROJECT_ROOT / 'data' / 'Walmart_Sales.csv'\n",
    "METADATA_PATH = PROJECT_ROOT / 'outputs' / 'metadata.json'\n",
    "OUTPUTS_DIR = PROJECT_ROOT / 'outputs'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48241d46",
   "metadata": {
    "papermill": {
     "duration": 0.006137,
     "end_time": "2026-02-02T11:42:30.488850",
     "exception": false,
     "start_time": "2026-02-02T11:42:30.482713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1) Cargar metadata (split + features)\n",
    "Esto garantiza consistencia entre modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "execution_state": "idle",
   "id": "2d83cde3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:42:30.499001Z",
     "iopub.status.busy": "2026-02-02T11:42:30.498630Z",
     "iopub.status.idle": "2026-02-02T11:42:30.503122Z",
     "shell.execute_reply": "2026-02-02T11:42:30.502609Z"
    },
    "papermill": {
     "duration": 0.010717,
     "end_time": "2026-02-02T11:42:30.504060",
     "exception": false,
     "start_time": "2026-02-02T11:42:30.493343",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split: {'train_start': '2010-02-05', 'train_end': '2011-12-02', 'val_start': '2011-12-09', 'val_end': '2012-01-27', 'test_start': '2012-02-03', 'test_end': '2012-10-26'}\n",
      "N features: 19\n"
     ]
    }
   ],
   "source": [
    "metadata = json.loads(METADATA_PATH.read_text(encoding='utf-8'))\n",
    "split = metadata['split']\n",
    "feature_cols = metadata['features']\n",
    "print('Split:', split)\n",
    "print('N features:', len(feature_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd048bc",
   "metadata": {
    "papermill": {
     "duration": 0.004399,
     "end_time": "2026-02-02T11:42:30.512877",
     "exception": false,
     "start_time": "2026-02-02T11:42:30.508478",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 2) Carga de datos + features\n",
    "- Parseo/orden\n",
    "- Construcción de lags/rolling (sin leakage)\n",
    "- Exógenas alineadas por fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "execution_state": "idle",
   "id": "d9f97a9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:42:30.522703Z",
     "iopub.status.busy": "2026-02-02T11:42:30.522448Z",
     "iopub.status.idle": "2026-02-02T11:42:30.568091Z",
     "shell.execute_reply": "2026-02-02T11:42:30.567410Z"
    },
    "papermill": {
     "duration": 0.051682,
     "end_time": "2026-02-02T11:42:30.569142",
     "exception": false,
     "start_time": "2026-02-02T11:42:30.517460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4095, 22)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = load_data(DATA_PATH)\n",
    "df_feat, _ = make_features(df, add_calendar=True)\n",
    "\n",
    "# Importante: para entrenar, debes decidir cómo tratar NaNs creados por lags/rolling\n",
    "# Opción típica: descartar filas con NaNs en features (por store al inicio)\n",
    "model_df = df_feat.dropna(subset=feature_cols + ['Weekly_Sales']).copy()\n",
    "model_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a122d5d6",
   "metadata": {
    "papermill": {
     "duration": 0.004531,
     "end_time": "2026-02-02T11:42:30.578484",
     "exception": false,
     "start_time": "2026-02-02T11:42:30.573953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3) Split temporal\n",
    "Reutiliza exactamente el split definido en el notebook 00."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "execution_state": "idle",
   "id": "badc5119",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:42:30.588602Z",
     "iopub.status.busy": "2026-02-02T11:42:30.588332Z",
     "iopub.status.idle": "2026-02-02T11:42:30.606595Z",
     "shell.execute_reply": "2026-02-02T11:42:30.606066Z"
    },
    "papermill": {
     "duration": 0.024296,
     "end_time": "2026-02-02T11:42:30.607486",
     "exception": false,
     "start_time": "2026-02-02T11:42:30.583190",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1980 360 1755\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df, split_cfg = temporal_split(df)\n",
    "\n",
    "# Aplicar el split sobre model_df (ya sin NaNs por lags)\n",
    "train = model_df[model_df['Date'].between(split_cfg.train_start, split_cfg.train_end)].copy()\n",
    "val = model_df[model_df['Date'].between(split_cfg.val_start, split_cfg.val_end)].copy()\n",
    "test = model_df[model_df['Date'].between(split_cfg.test_start, split_cfg.test_end)].copy()\n",
    "\n",
    "print(len(train), len(val), len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2403b7a",
   "metadata": {
    "papermill": {
     "duration": 0.004605,
     "end_time": "2026-02-02T11:42:30.616830",
     "exception": false,
     "start_time": "2026-02-02T11:42:30.612225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4) Entrenamiento del modelo\n",
    "Implementación Transformer global con covariables exógenas.\n",
    "Incluye representaciones de Store (one-hot o embedding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "execution_state": "idle",
   "id": "7a76f46d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:42:30.626892Z",
     "iopub.status.busy": "2026-02-02T11:42:30.626623Z",
     "iopub.status.idle": "2026-02-02T11:42:30.630408Z",
     "shell.execute_reply": "2026-02-02T11:42:30.629831Z"
    },
    "papermill": {
     "duration": 0.010004,
     "end_time": "2026-02-02T11:42:30.631336",
     "exception": false,
     "start_time": "2026-02-02T11:42:30.621332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: implementar entrenamiento Transformer con exógenas\n",
    "# Debe producir predicciones para TEST (ideal: también para VAL).\n",
    "y_pred_test = np.full(shape=len(test), fill_value=test['Weekly_Sales'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48213bf3",
   "metadata": {
    "papermill": {
     "duration": 0.004462,
     "end_time": "2026-02-02T11:42:30.640271",
     "exception": false,
     "start_time": "2026-02-02T11:42:30.635809",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5) Métricas (MAE, RMSE, sMAPE)\n",
    "Se reporta:\n",
    "- Global\n",
    "- Por store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "execution_state": "idle",
   "id": "026c22ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:42:30.651765Z",
     "iopub.status.busy": "2026-02-02T11:42:30.651333Z",
     "iopub.status.idle": "2026-02-02T11:43:01.372150Z",
     "shell.execute_reply": "2026-02-02T11:43:01.371326Z"
    },
    "papermill": {
     "duration": 30.732209,
     "end_time": "2026-02-02T11:43:01.377939",
     "exception": false,
     "start_time": "2026-02-02T11:42:30.645730",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WINDOW</th>\n",
       "      <th>n_train</th>\n",
       "      <th>n_val</th>\n",
       "      <th>n_test</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26</td>\n",
       "      <td>810</td>\n",
       "      <td>360</td>\n",
       "      <td>1755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>1395</td>\n",
       "      <td>360</td>\n",
       "      <td>1755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   WINDOW  n_train  n_val  n_test\n",
       "0      52        0      0    1755\n",
       "1      26      810    360    1755\n",
       "2      13     1395    360    1755"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Transformer] WINDOW forzada: 26 (train/val/test: 810/360/1755)\n",
      "\n",
      "=== Transformer config {'d_model': 64, 'nhead': 4, 'num_layers': 2, 'dropout': 0.1, 'lr': 0.001} ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 | train=0.0578 | val=0.5159 | lr=1.00e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 | train=0.0383 | val=0.5500 | lr=5.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200 | train=0.0264 | val=0.5677 | lr=5.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 19 (best val 0.3670)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Transformer config {'d_model': 64, 'nhead': 4, 'num_layers': 2, 'dropout': 0.2, 'lr': 0.0003} ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 | train=0.1074 | val=0.3016 | lr=3.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 | train=0.0795 | val=0.2925 | lr=3.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200 | train=0.0518 | val=0.3070 | lr=1.50e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200 | train=0.0403 | val=0.3041 | lr=7.50e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 23 (best val 0.2724)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Transformer config {'d_model': 128, 'nhead': 8, 'num_layers': 4, 'dropout': 0.1, 'lr': 0.0003} ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 | train=0.0975 | val=0.3997 | lr=3.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 | train=0.0617 | val=0.5137 | lr=1.50e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200 | train=0.0260 | val=0.4057 | lr=7.50e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 18 (best val 0.3882)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Transformer config {'d_model': 128, 'nhead': 8, 'num_layers': 2, 'dropout': 0.2, 'lr': 0.001} ===\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 | train=0.0808 | val=0.3927 | lr=1.00e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 | train=0.0507 | val=0.6803 | lr=1.00e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200 | train=0.0275 | val=0.4293 | lr=5.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200 | train=0.0222 | val=0.4883 | lr=2.50e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 23 (best val 0.3446)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>sMAPE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transformer_exog__dm64__nh4__nl2__do0.1__lr0.001</td>\n",
       "      <td>95112.183075</td>\n",
       "      <td>126195.599627</td>\n",
       "      <td>10.259245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>transformer_exog__dm128__nh8__nl4__do0.1__lr0....</td>\n",
       "      <td>111218.728259</td>\n",
       "      <td>142691.357145</td>\n",
       "      <td>14.172786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transformer_exog__dm128__nh8__nl2__do0.2__lr0.001</td>\n",
       "      <td>135133.266824</td>\n",
       "      <td>193913.299095</td>\n",
       "      <td>14.272250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>transformer_exog__dm64__nh4__nl2__do0.2__lr0.0003</td>\n",
       "      <td>127755.594059</td>\n",
       "      <td>168160.650048</td>\n",
       "      <td>15.426463</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               model            MAE  \\\n",
       "0   transformer_exog__dm64__nh4__nl2__do0.1__lr0.001   95112.183075   \n",
       "1  transformer_exog__dm128__nh8__nl4__do0.1__lr0....  111218.728259   \n",
       "2  transformer_exog__dm128__nh8__nl2__do0.2__lr0.001  135133.266824   \n",
       "3  transformer_exog__dm64__nh4__nl2__do0.2__lr0.0003  127755.594059   \n",
       "\n",
       "            RMSE      sMAPE  \n",
       "0  126195.599627  10.259245  \n",
       "1  142691.357145  14.172786  \n",
       "2  193913.299095  14.272250  \n",
       "3  168160.650048  15.426463  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from warnings import filterwarnings\n",
    "\n",
    "\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "try:\n",
    "\n",
    "    import torch\n",
    "\n",
    "    from torch import nn\n",
    "\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "except Exception as exc:\n",
    "\n",
    "    raise ImportError(\n",
    "\n",
    "        \"PyTorch no está instalado. Instala con: pip install torch\"\n",
    "\n",
    "    ) from exc\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "\n",
    "\n",
    "\n",
    "# Helper para features autoregresivas (sin leakage)\n",
    "\n",
    "def _compute_feature_vector(y_hist, date, exog_row, cfg):\n",
    "\n",
    "    lags = cfg[\"lags\"]\n",
    "\n",
    "    rollings = cfg[\"rollings\"]\n",
    "\n",
    "    add_calendar = bool(cfg.get(\"add_calendar\", True))\n",
    "\n",
    "    exog_cols = cfg[\"exog_cols\"]\n",
    "\n",
    "    feat = {}\n",
    "\n",
    "    for k in lags:\n",
    "\n",
    "        feat[f\"lag_{k}\"] = y_hist[-k] if len(y_hist) >= k else np.nan\n",
    "\n",
    "    for w in rollings:\n",
    "\n",
    "        if len(y_hist) >= w:\n",
    "\n",
    "            window = np.array(y_hist[-w:], dtype=float)\n",
    "\n",
    "            feat[f\"roll_mean_{w}\"] = float(window.mean())\n",
    "\n",
    "            feat[f\"roll_std_{w}\"] = float(window.std(ddof=0))\n",
    "\n",
    "        else:\n",
    "\n",
    "            feat[f\"roll_mean_{w}\"] = np.nan\n",
    "\n",
    "            feat[f\"roll_std_{w}\"] = np.nan\n",
    "\n",
    "    for c in exog_cols:\n",
    "\n",
    "        feat[c] = float(exog_row[c])\n",
    "\n",
    "    if add_calendar:\n",
    "        iso = pd.Timestamp(date).isocalendar()\n",
    "        week_val = iso.week if hasattr(iso, 'week') else iso[1]\n",
    "        year_val = iso.year if hasattr(iso, 'year') else iso[0]\n",
    "        feat[\"weekofyear\"] = int(week_val)\n",
    "        feat[\"month\"] = int(pd.Timestamp(date).month)\n",
    "        feat[\"year\"] = int(year_val)\n",
    "\n",
    "    vec = [feat.get(c, np.nan) for c in feature_cols]\n",
    "\n",
    "    return vec\n",
    "\n",
    "\n",
    "\n",
    "# Configuración base + search space\n",
    "\n",
    "EPOCHS_MAX = 200\n",
    "\n",
    "PATIENCE_ES = 15\n",
    "\n",
    "MIN_DELTA = 1e-4\n",
    "\n",
    "CLIP_NORM = 1.0\n",
    "\n",
    "WEIGHT_DECAY = 1e-4\n",
    "\n",
    "REDUCE_LR_FACTOR = 0.5\n",
    "\n",
    "REDUCE_LR_PATIENCE = 5\n",
    "\n",
    "EMB_DIM = 16\n",
    "\n",
    "# Secuencia autoregresiva (ventana).\n",
    "# Nota: con splits por fecha, una ventana demasiado grande puede dejar el train vacío (num_samples=0).\n",
    "WINDOW_CANDIDATES = [52, 26, 13]\n",
    "FORCE_WINDOW = 26  # ventana fija (pon None para auto)\n",
    "WINDOW = int(FORCE_WINDOW) if FORCE_WINDOW is not None else int(WINDOW_CANDIDATES[0])\n",
    "\n",
    "\n",
    "\n",
    "transformer_search = [\n",
    "\n",
    "    {\"d_model\": 64, \"nhead\": 4, \"num_layers\": 2, \"dropout\": 0.1, \"lr\": 1e-3},\n",
    "\n",
    "    {\"d_model\": 64, \"nhead\": 4, \"num_layers\": 2, \"dropout\": 0.2, \"lr\": 3e-4},\n",
    "\n",
    "    {\"d_model\": 128, \"nhead\": 8, \"num_layers\": 4, \"dropout\": 0.1, \"lr\": 3e-4},\n",
    "\n",
    "    {\"d_model\": 128, \"nhead\": 8, \"num_layers\": 2, \"dropout\": 0.2, \"lr\": 1e-3},\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Anti-leakage\n",
    "\n",
    "assert split_cfg.train_end < split_cfg.val_start < split_cfg.val_end < split_cfg.test_start <= split_cfg.test_end\n",
    "\n",
    "\n",
    "\n",
    "# Mapping Store -> idx para embeddings\n",
    "\n",
    "stores_sorted = sorted(model_df[\"Store\"].unique())\n",
    "\n",
    "store_to_idx = {s: i for i, s in enumerate(stores_sorted)}\n",
    "\n",
    "num_stores = len(store_to_idx)\n",
    "\n",
    "\n",
    "\n",
    "feature_cols_model = feature_cols\n",
    "DEFAULT_LAGS = sorted({int(c.split(\"_\")[1]) for c in feature_cols if c.startswith(\"lag_\")})\n",
    "DEFAULT_ROLLINGS = sorted({int(c.split(\"_\")[2]) for c in feature_cols if c.startswith(\"roll_mean_\")})\n",
    "EXOG_COLUMNS = [c for c in feature_cols if not c.startswith(\"lag_\") and not c.startswith(\"roll_\")]\n",
    "\n",
    "\n",
    "\n",
    "# Escaladores SOLO con train\n",
    "\n",
    "scaler_x = StandardScaler()\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "train_fit = train.copy()\n",
    "\n",
    "scaler_x.fit(train_fit[feature_cols_model].values)\n",
    "\n",
    "scaler_y.fit(train_fit[[\"Weekly_Sales\"]].values)\n",
    "\n",
    "assert hasattr(scaler_x, \"mean_\") and hasattr(scaler_y, \"mean_\")\n",
    "\n",
    "\n",
    "\n",
    "def build_sequences(df_in: pd.DataFrame, window: int = WINDOW):\n",
    "\n",
    "    sequences, targets, dates, stores_idx = [], [], [], []\n",
    "\n",
    "    df_in = df_in.sort_values([\"Store\", \"Date\"]).copy()\n",
    "\n",
    "    for store, g in df_in.groupby(\"Store\"):\n",
    "\n",
    "        g = g.sort_values(\"Date\")\n",
    "\n",
    "        X = scaler_x.transform(g[feature_cols_model].values)\n",
    "\n",
    "        y = scaler_y.transform(g[[\"Weekly_Sales\"]].values).ravel()\n",
    "\n",
    "        date_arr = g[\"Date\"].values\n",
    "\n",
    "        s_idx = store_to_idx[int(store)]\n",
    "\n",
    "        for t in range(window, len(g)):\n",
    "\n",
    "            sequences.append(X[t - window : t])\n",
    "\n",
    "            targets.append(y[t])\n",
    "\n",
    "            dates.append(date_arr[t])\n",
    "\n",
    "            stores_idx.append(s_idx)\n",
    "\n",
    "    return (\n",
    "\n",
    "        np.array(sequences),\n",
    "\n",
    "        np.array(targets),\n",
    "\n",
    "        np.array(dates),\n",
    "\n",
    "        np.array(stores_idx, dtype=int),\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def _try_build_for_window(window: int):\n",
    "    X_seq, y_seq, d_seq, s_seq = build_sequences(model_df, window=window)\n",
    "    train_mask = (d_seq >= split_cfg.train_start) & (d_seq <= split_cfg.train_end)\n",
    "    val_mask = (d_seq >= split_cfg.val_start) & (d_seq <= split_cfg.val_end)\n",
    "    test_mask = (d_seq >= split_cfg.test_start) & (d_seq <= split_cfg.test_end)\n",
    "    return X_seq, y_seq, d_seq, s_seq, train_mask, val_mask, test_mask\n",
    "\n",
    "# Reporte de muestras por ventana (útil para justificar decisiones)\n",
    "window_report = []\n",
    "for w in WINDOW_CANDIDATES:\n",
    "    _X, _y, _d, _s, _tr, _va, _te = _try_build_for_window(w)\n",
    "    window_report.append({\"WINDOW\": w, \"n_train\": int(_tr.sum()), \"n_val\": int(_va.sum()), \"n_test\": int(_te.sum())})\n",
    "window_report_df = pd.DataFrame(window_report)\n",
    "display(window_report_df)\n",
    "\n",
    "# Selección de WINDOW\n",
    "if FORCE_WINDOW is not None:\n",
    "    WINDOW = int(FORCE_WINDOW)\n",
    "    if WINDOW not in WINDOW_CANDIDATES:\n",
    "        raise ValueError(f\"FORCE_WINDOW={WINDOW} no está en WINDOW_CANDIDATES={WINDOW_CANDIDATES}\")\n",
    "    row = window_report_df[window_report_df[\"WINDOW\"] == WINDOW].iloc[0]\n",
    "    if int(row.n_train) <= 0 or int(row.n_val) <= 0 or int(row.n_test) <= 0:\n",
    "        raise ValueError(\n",
    "            \"WINDOW forzada no es viable (alguna partición queda vacía). \"\n",
    "            f\"FORCE_WINDOW={WINDOW}; conteos: train/val/test={int(row.n_train)}/{int(row.n_val)}/{int(row.n_test)}\"\n",
    "        )\n",
    "    print(f\"[Transformer] WINDOW forzada: {WINDOW} (train/val/test: {int(row.n_train)}/{int(row.n_val)}/{int(row.n_test)})\")\n",
    "else:\n",
    "    WINDOW = None\n",
    "    for w in WINDOW_CANDIDATES:\n",
    "        row = window_report_df[window_report_df[\"WINDOW\"] == w].iloc[0]\n",
    "        if int(row.n_train) > 0 and int(row.n_val) > 0 and int(row.n_test) > 0:\n",
    "            WINDOW = int(w)\n",
    "            break\n",
    "    if WINDOW is None:\n",
    "        msg = (\n",
    "            \"No hay muestras tras construir secuencias para ninguna WINDOW candidata.\\n\"\n",
    "            f\"Candidatas: {WINDOW_CANDIDATES}\\n\"\n",
    "            f\"Split train: {split_cfg.train_start} -> {split_cfg.train_end}\\n\"\n",
    "            f\"Split val:   {split_cfg.val_start} -> {split_cfg.val_end}\\n\"\n",
    "            f\"Split test:  {split_cfg.test_start} -> {split_cfg.test_end}\\n\"\n",
    "            \"Sugerencia: reduce WINDOW, revisa que Date sea datetime64, o ajusta el split.\"\n",
    "        )\n",
    "        raise ValueError(msg)\n",
    "    row = window_report_df[window_report_df[\"WINDOW\"] == WINDOW].iloc[0]\n",
    "    print(f\"[Transformer] WINDOW seleccionada automáticamente: {WINDOW} (train/val/test: {int(row.n_train)}/{int(row.n_val)}/{int(row.n_test)})\")\n",
    "\n",
    "# Construir secuencias con la WINDOW elegida\n",
    "X_seq, y_seq, d_seq, s_seq, train_mask, val_mask, test_mask = _try_build_for_window(WINDOW)\n",
    "\n",
    "# Split por fecha (sin leakage)\n",
    "assert not (train_mask & val_mask).any() and not (train_mask & test_mask).any() and not (val_mask & test_mask).any()\n",
    "\n",
    "X_train, y_train, s_train = X_seq[train_mask], y_seq[train_mask], s_seq[train_mask]\n",
    "\n",
    "X_val, y_val, s_val = X_seq[val_mask], y_seq[val_mask], s_seq[val_mask]\n",
    "\n",
    "X_test_seq, y_test_seq, s_test_seq = X_seq[test_mask], y_seq[test_mask], s_seq[test_mask]\n",
    "\n",
    "test_dates = d_seq[test_mask]\n",
    "\n",
    "test_stores = s_seq[test_mask]\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "\n",
    "    TensorDataset(\n",
    "\n",
    "        torch.tensor(X_train, dtype=torch.float32),\n",
    "\n",
    "        torch.tensor(s_train, dtype=torch.long),\n",
    "\n",
    "        torch.tensor(y_train, dtype=torch.float32),\n",
    "\n",
    "    ),\n",
    "\n",
    "    batch_size=64,\n",
    "\n",
    "    shuffle=True,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "val_loader = DataLoader(\n",
    "\n",
    "    TensorDataset(\n",
    "\n",
    "        torch.tensor(X_val, dtype=torch.float32),\n",
    "\n",
    "        torch.tensor(s_val, dtype=torch.long),\n",
    "\n",
    "        torch.tensor(y_val, dtype=torch.float32),\n",
    "\n",
    "    ),\n",
    "\n",
    "    batch_size=64,\n",
    "\n",
    "    shuffle=False,\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n",
    "class TransformerRegressor(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size: int, d_model: int, nhead: int, num_layers: int, dropout: float, max_len: int, emb_dim: int, num_stores: int):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_proj = nn.Linear(input_size, d_model)\n",
    "\n",
    "        self.store_emb = nn.Embedding(num_stores, emb_dim)\n",
    "\n",
    "        self.store_proj = nn.Linear(emb_dim, d_model)\n",
    "\n",
    "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout, batch_first=True)\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, 1)\n",
    "\n",
    "        self.max_len = max_len\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, store_idx):\n",
    "\n",
    "        b, t, _ = x.size()\n",
    "\n",
    "        pos_idx = torch.arange(t, device=x.device)\n",
    "\n",
    "        pos = self.pos_emb(pos_idx).unsqueeze(0).expand(b, t, -1)\n",
    "\n",
    "        emb = self.store_proj(self.store_emb(store_idx)).unsqueeze(1).expand(b, t, -1)\n",
    "\n",
    "        h = self.input_proj(x) + emb + pos\n",
    "\n",
    "        h = self.encoder(h)\n",
    "\n",
    "        out = h[:, -1, :]\n",
    "\n",
    "        return self.fc(out).squeeze(-1)\n",
    "\n",
    "\n",
    "\n",
    "def train_eval(config_run: dict):\n",
    "\n",
    "    model = TransformerRegressor(\n",
    "\n",
    "        input_size=X_train.shape[-1],\n",
    "\n",
    "        d_model=config_run[\"d_model\"],\n",
    "\n",
    "        nhead=config_run[\"nhead\"],\n",
    "\n",
    "        num_layers=config_run[\"num_layers\"],\n",
    "\n",
    "        dropout=config_run[\"dropout\"],\n",
    "\n",
    "        max_len=WINDOW,\n",
    "\n",
    "        emb_dim=EMB_DIM,\n",
    "\n",
    "        num_stores=num_stores,\n",
    "\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "\n",
    "        model.parameters(), lr=config_run[\"lr\"], weight_decay=WEIGHT_DECAY\n",
    "\n",
    "    )\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "\n",
    "        optimizer, mode=\"min\", factor=REDUCE_LR_FACTOR, patience=REDUCE_LR_PATIENCE, verbose=False\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "\n",
    "    best_state = None\n",
    "\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    history = []\n",
    "\n",
    "\n",
    "\n",
    "    for epoch in range(EPOCHS_MAX):\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        train_losses = []\n",
    "\n",
    "        for xb, sb, yb in train_loader:\n",
    "\n",
    "            xb, sb, yb = xb.to(device), sb.to(device), yb.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = model(xb, sb)\n",
    "\n",
    "            loss = criterion(preds, yb)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        val_losses = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for xb, sb, yb in val_loader:\n",
    "\n",
    "                xb, sb, yb = xb.to(device), sb.to(device), yb.to(device)\n",
    "\n",
    "                preds = model(xb, sb)\n",
    "\n",
    "                val_losses.append(criterion(preds, yb).item())\n",
    "\n",
    "\n",
    "\n",
    "        train_loss = float(np.mean(train_losses))\n",
    "\n",
    "        val_loss = float(np.mean(val_losses)) if val_losses else train_loss\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        history.append((epoch + 1, train_loss, val_loss, current_lr))\n",
    "\n",
    "\n",
    "\n",
    "        if val_loss + MIN_DELTA < best_val:\n",
    "\n",
    "            best_val = val_loss\n",
    "\n",
    "            best_state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
    "\n",
    "            epochs_no_improve = 0\n",
    "\n",
    "        else:\n",
    "\n",
    "            epochs_no_improve += 1\n",
    "\n",
    "\n",
    "\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{EPOCHS_MAX} | train={train_loss:.4f} | val={val_loss:.4f} | lr={current_lr:.2e}\")\n",
    "\n",
    "\n",
    "\n",
    "        if epochs_no_improve >= PATIENCE_ES:\n",
    "\n",
    "            print(f\"Early stopping at epoch {epoch+1} (best val {best_val:.4f})\")\n",
    "\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "    if best_state is not None:\n",
    "\n",
    "        model.load_state_dict({k: v.to(device) for k, v in best_state.items()})\n",
    "\n",
    "\n",
    "\n",
    "    return model, best_val, history\n",
    "\n",
    "\n",
    "\n",
    "def predict_test(model):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    for store, g_future in test.groupby(\"Store\"):\n",
    "\n",
    "        g_future = g_future.sort_values(\"Date\")\n",
    "\n",
    "        g_hist = train[train[\"Store\"] == store].sort_values(\"Date\")\n",
    "\n",
    "        if g_hist.empty:\n",
    "\n",
    "            continue\n",
    "\n",
    "        y_hist = g_hist[\"Weekly_Sales\"].tolist()\n",
    "\n",
    "        feat_hist = []\n",
    "\n",
    "        for _, row in g_hist.iterrows():\n",
    "\n",
    "            feat_hist.append(\n",
    "\n",
    "                _compute_feature_vector(\n",
    "\n",
    "                    y_hist[: g_hist.index.get_loc(row.name) + 1],\n",
    "\n",
    "                    row[\"Date\"],\n",
    "\n",
    "                    row,\n",
    "\n",
    "                    {'lags': DEFAULT_LAGS, 'rollings': DEFAULT_ROLLINGS, 'add_calendar': True, 'exog_cols': EXOG_COLUMNS, 'feature_cols': feature_cols},\n",
    "\n",
    "                )\n",
    "\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        store_preds = []\n",
    "\n",
    "        s_idx = store_to_idx[int(store)]\n",
    "\n",
    "        for _, row in g_future.iterrows():\n",
    "\n",
    "            feat_vec = _compute_feature_vector(\n",
    "\n",
    "                y_hist,\n",
    "\n",
    "                row[\"Date\"],\n",
    "\n",
    "                row,\n",
    "\n",
    "                {'lags': DEFAULT_LAGS, 'rollings': DEFAULT_ROLLINGS, 'add_calendar': True, 'exog_cols': EXOG_COLUMNS, 'feature_cols': feature_cols},\n",
    "\n",
    "            )\n",
    "\n",
    "            feat_hist.append(feat_vec)\n",
    "\n",
    "            seq = np.array(feat_hist[-WINDOW :], dtype=float)\n",
    "\n",
    "            seq = np.nan_to_num(seq, nan=train[\"Weekly_Sales\"].mean())\n",
    "\n",
    "            seq_scaled = scaler_x.transform(seq)\n",
    "\n",
    "            xb = torch.tensor(seq_scaled, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "            sb = torch.tensor([s_idx], dtype=torch.long).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                yhat_scaled = model(xb, sb).cpu().numpy().ravel()[0]\n",
    "\n",
    "            yhat = scaler_y.inverse_transform([[yhat_scaled]])[0][0]\n",
    "\n",
    "            y_hist.append(float(yhat))\n",
    "\n",
    "            store_preds.append(float(yhat))\n",
    "\n",
    "\n",
    "\n",
    "        preds.append(\n",
    "\n",
    "            pd.DataFrame(\n",
    "\n",
    "                {\n",
    "\n",
    "                    \"Store\": g_future[\"Store\"].values,\n",
    "\n",
    "                    \"Date\": g_future[\"Date\"].values,\n",
    "\n",
    "                    \"y_pred\": store_preds,\n",
    "\n",
    "                }\n",
    "\n",
    "            )\n",
    "\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    if not preds:\n",
    "\n",
    "        return pd.DataFrame(columns=[\"Store\", \"Date\", \"y_pred\"])\n",
    "\n",
    "    return pd.concat(preds, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for cfg in transformer_search:\n",
    "\n",
    "    print(f\"\\n=== Transformer config {cfg} ===\")\n",
    "\n",
    "    model, best_val, history = train_eval(cfg)\n",
    "\n",
    "    pred_df = predict_test(model)\n",
    "\n",
    "    pred_df = pred_df.merge(\n",
    "\n",
    "        test[[\"Store\", \"Date\", \"Weekly_Sales\"]], on=[\"Store\", \"Date\"], how=\"left\"\n",
    "\n",
    "    ).rename(columns={\"Weekly_Sales\": \"y_true\"})\n",
    "\n",
    "    name = f\"transformer_exog__dm{cfg['d_model']}__nh{cfg['nhead']}__nl{cfg['num_layers']}__do{cfg['dropout']}__lr{cfg['lr']}\"\n",
    "\n",
    "    pred_df[\"model\"] = name\n",
    "\n",
    "    metrics = compute_metrics(pred_df[\"y_true\"].values, pred_df[\"y_pred\"].values)\n",
    "\n",
    "    results.append({\"cfg\": cfg, \"metrics\": metrics, \"name\": name})\n",
    "\n",
    "    save_outputs(\n",
    "\n",
    "        model_name=name,\n",
    "\n",
    "        predictions=pred_df,\n",
    "\n",
    "        metrics_global=pd.DataFrame([{**{\"model\": name}, **metrics}]),\n",
    "\n",
    "        metrics_by_store=pred_df.groupby(\"Store\").apply(\n",
    "\n",
    "            lambda g: pd.Series(compute_metrics(g[\"y_true\"].values, g[\"y_pred\"].values))\n",
    "\n",
    "        ).reset_index().assign(model=name),\n",
    "\n",
    "        output_dir=OUTPUTS_DIR,\n",
    "\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame([\n",
    "\n",
    "    {\"model\": r[\"name\"], \"MAE\": r[\"metrics\"][\"MAE\"], \"RMSE\": r[\"metrics\"][\"RMSE\"], \"sMAPE\": r[\"metrics\"][\"sMAPE\"]}\n",
    "\n",
    "    for r in results\n",
    "\n",
    "])\n",
    "\n",
    "results_df = results_df.sort_values([\"sMAPE\", \"MAE\"]).reset_index(drop=True)\n",
    "\n",
    "best_model_name = results_df.loc[0, \"model\"]\n",
    "\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91980d50",
   "metadata": {
    "papermill": {
     "duration": 0.00536,
     "end_time": "2026-02-02T11:43:01.388721",
     "exception": false,
     "start_time": "2026-02-02T11:43:01.383361",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5) Métricas (MAE, RMSE, sMAPE)\n",
    "Se reporta:\n",
    "- Global\n",
    "- Por store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "execution_state": "idle",
   "id": "a91f5e1e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:43:01.400119Z",
     "iopub.status.busy": "2026-02-02T11:43:01.399794Z",
     "iopub.status.idle": "2026-02-02T11:43:07.507460Z",
     "shell.execute_reply": "2026-02-02T11:43:07.506803Z"
    },
    "papermill": {
     "duration": 6.114727,
     "end_time": "2026-02-02T11:43:07.508610",
     "exception": false,
     "start_time": "2026-02-02T11:43:01.393883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-entrenando mejor config para outputs: {'d_model': 64, 'nhead': 4, 'num_layers': 2, 'dropout': 0.1, 'lr': 0.001}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 | train=0.0812 | val=0.4016 | lr=1.00e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 | train=0.0433 | val=0.3636 | lr=1.00e-03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200 | train=0.0253 | val=0.3848 | lr=5.00e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200 | train=0.0245 | val=0.3977 | lr=2.50e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 22 (best val 0.3320)\n"
     ]
    }
   ],
   "source": [
    "# Re-entrenar mejor config para guardar outputs consistentes\n",
    "best_cfg = next(r[\"cfg\"] for r in results if r[\"name\"] == best_model_name)\n",
    "print(\"Re-entrenando mejor config para outputs:\", best_cfg)\n",
    "\n",
    "best_model, best_val, _ = train_eval(best_cfg)\n",
    "\n",
    "pred_df = predict_test(best_model)\n",
    "pred_df = pred_df.merge(\n",
    "    test[[\"Store\", \"Date\", \"Weekly_Sales\"]], on=[\"Store\", \"Date\"], how=\"left\"\n",
    ").rename(columns={\"Weekly_Sales\": \"y_true\"})\n",
    "pred_df[\"model\"] = best_model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "execution_state": "idle",
   "id": "3f51d4b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:43:07.521501Z",
     "iopub.status.busy": "2026-02-02T11:43:07.521195Z",
     "iopub.status.idle": "2026-02-02T11:43:07.530611Z",
     "shell.execute_reply": "2026-02-02T11:43:07.530046Z"
    },
    "papermill": {
     "duration": 0.017008,
     "end_time": "2026-02-02T11:43:07.531537",
     "exception": false,
     "start_time": "2026-02-02T11:43:07.514529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor config (ordenada por sMAPE):\n",
      "model    transformer_exog__dm64__nh4__nl2__do0.1__lr0.001\n",
      "MAE                                          95112.183075\n",
      "RMSE                                        126195.599627\n",
      "sMAPE                                           10.259245\n",
      "Name: 0, dtype: object\n",
      "\n",
      "Baseline (EPOCHS=20) encontrado:\n",
      "                                               model           MAE  \\\n",
      "0  transformer_exog__dm128__nh8__nl4__do0.1__lr0....  62559.516655   \n",
      "\n",
      "           RMSE     sMAPE  \n",
      "0  80319.226581  7.016417  \n",
      "Comparación sMAPE delta: 3.2428278221706\n"
     ]
    }
   ],
   "source": [
    "# Resumen rápido: mejor config y (si existe) baseline de 20 epochs\n",
    "if \"metrics_global_df\" in globals():\n",
    "    best_row = metrics_global_df.iloc[0]\n",
    "elif \"results_df\" in globals():\n",
    "    best_row = results_df.iloc[0]\n",
    "else:\n",
    "    raise ValueError(\"No hay resultados para mostrar.\")\n",
    "\n",
    "print(\"Mejor config (ordenada por sMAPE):\")\n",
    "print(best_row)\n",
    "\n",
    "# Intentar cargar baseline (si ya existe en outputs)\n",
    "if \"baseline_path\" not in globals():\n",
    "    import pathlib\n",
    "    baseline_path = pathlib.Path(OUTPUTS_DIR) / \"metrics\" / \"transformer_exog_metrics_global.csv\"\n",
    "\n",
    "if baseline_path.exists():\n",
    "    baseline_df = pd.read_csv(baseline_path)\n",
    "    print(\"\\nBaseline (EPOCHS=20) encontrado:\")\n",
    "    print(baseline_df)\n",
    "    print(\"Comparación sMAPE delta:\", float(best_row[\"sMAPE\"]) - float(baseline_df.loc[0, \"sMAPE\"]))\n",
    "else:\n",
    "    print(\"\\nBaseline (EPOCHS=20) no encontrado; ejecuta el baseline para comparar.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6705a3",
   "metadata": {
    "papermill": {
     "duration": 0.005548,
     "end_time": "2026-02-02T11:43:07.542790",
     "exception": false,
     "start_time": "2026-02-02T11:43:07.537242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6) Guardado de outputs estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "execution_state": "idle",
   "id": "8717a65c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:43:07.554669Z",
     "iopub.status.busy": "2026-02-02T11:43:07.554385Z",
     "iopub.status.idle": "2026-02-02T11:43:07.582259Z",
     "shell.execute_reply": "2026-02-02T11:43:07.581708Z"
    },
    "papermill": {
     "duration": 0.035016,
     "end_time": "2026-02-02T11:43:07.583221",
     "exception": false,
     "start_time": "2026-02-02T11:43:07.548205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                              model            MAE  \\\n",
       " 0  transformer_exog__dm64__nh4__nl2__do0.1__lr0.001  102597.771243   \n",
       " \n",
       "             RMSE      sMAPE      WAPE  \n",
       " 0  137821.155509  10.849608  0.098326  ,\n",
       "                                               model  Store            MAE  \\\n",
       " 0  transformer_exog__dm64__nh4__nl2__do0.1__lr0.001      1  141721.285126   \n",
       " 1  transformer_exog__dm64__nh4__nl2__do0.1__lr0.001      2  115566.480548   \n",
       " 2  transformer_exog__dm64__nh4__nl2__do0.1__lr0.001      3   36653.131510   \n",
       " 3  transformer_exog__dm64__nh4__nl2__do0.1__lr0.001      4  109806.163420   \n",
       " 4  transformer_exog__dm64__nh4__nl2__do0.1__lr0.001      5   54412.150799   \n",
       " \n",
       "             RMSE      sMAPE      WAPE  \n",
       " 0  170548.724544   9.079771  0.088465  \n",
       " 1  145903.086920   6.088872  0.060433  \n",
       " 2   48110.459505   8.094447  0.086407  \n",
       " 3  138641.377560   4.987412  0.050472  \n",
       " 4   67176.554778  14.642130  0.163465  )"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Métricas globales y por tienda\n",
    "metrics_global = compute_metrics(pred_df[\"y_true\"].values, pred_df[\"y_pred\"].values)\n",
    "metrics_global_df = pd.DataFrame([\n",
    "    {\"model\": best_model_name, **metrics_global}\n",
    "])\n",
    "\n",
    "metrics_by_store_df = (\n",
    "    pred_df.groupby(\"Store\")\n",
    "    .apply(lambda g: pd.Series(compute_metrics(g[\"y_true\"].values, g[\"y_pred\"].values)))\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"MAE\", 1: \"RMSE\", 2: \"sMAPE\"})\n",
    ")\n",
    "metrics_by_store_df.insert(0, \"model\", best_model_name)\n",
    "metrics_by_store_df = metrics_by_store_df.sort_values(\"Store\")\n",
    "\n",
    "metrics_global_df, metrics_by_store_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "execution_state": "idle",
   "id": "cfc3a444",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:43:07.595458Z",
     "iopub.status.busy": "2026-02-02T11:43:07.595183Z",
     "iopub.status.idle": "2026-02-02T11:43:07.611442Z",
     "shell.execute_reply": "2026-02-02T11:43:07.610892Z"
    },
    "papermill": {
     "duration": 0.023544,
     "end_time": "2026-02-02T11:43:07.612516",
     "exception": false,
     "start_time": "2026-02-02T11:43:07.588972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'predictions': '/home/sagemaker-user/TFMAXEL/outputs/predictions/transformer_exog_predictions.csv',\n",
       " 'metrics_global': '/home/sagemaker-user/TFMAXEL/outputs/metrics/transformer_exog_metrics_global.csv',\n",
       " 'metrics_by_store': '/home/sagemaker-user/TFMAXEL/outputs/metrics/transformer_exog_metrics_by_store.csv'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths = save_outputs(\n",
    "    model_name=MODEL_NAME,\n",
    "    predictions=pred_df,\n",
    "    metrics_global=metrics_global_df,\n",
    "    metrics_by_store=metrics_by_store_df,\n",
    "    output_dir=OUTPUTS_DIR,\n",
    ")\n",
    "paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a98bd8",
   "metadata": {
    "papermill": {
     "duration": 0.005725,
     "end_time": "2026-02-02T11:43:07.624272",
     "exception": false,
     "start_time": "2026-02-02T11:43:07.618547",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 7) Figuras\n",
    "- 3 tiendas: real vs predicción en test\n",
    "- Distribución del error (`y_true - y_pred`)\n",
    "\n",
    "Guardar PNGs en `outputs/figures/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "execution_state": "idle",
   "id": "87d9b4e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-02T11:43:07.636348Z",
     "iopub.status.busy": "2026-02-02T11:43:07.636047Z",
     "iopub.status.idle": "2026-02-02T11:43:09.626218Z",
     "shell.execute_reply": "2026-02-02T11:43:09.625254Z"
    },
    "papermill": {
     "duration": 1.998467,
     "end_time": "2026-02-02T11:43:09.628269",
     "exception": false,
     "start_time": "2026-02-02T11:43:07.629802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "FIG_DIR = OUTPUTS_DIR / \"figures\"\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Selección de 3 tiendas (mayor media de ventas en test)\n",
    "top_stores = (\n",
    "    pred_df.groupby(\"Store\")[\"y_true\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(3)\n",
    "    .index\n",
    "    .tolist()\n",
    ")\n",
    "\n",
    "for store in top_stores:\n",
    "    g = pred_df[pred_df[\"Store\"] == store].sort_values(\"Date\")\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(g[\"Date\"], g[\"y_true\"], label=\"y_true\")\n",
    "    plt.plot(g[\"Date\"], g[\"y_pred\"], label=\"y_pred\")\n",
    "    plt.title(f\"Store {store} — Transformer\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Weekly_Sales\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / f\"{MODEL_NAME}_plot_store_{store}.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# Distribución de error\n",
    "errors = pred_df[\"y_true\"] - pred_df[\"y_pred\"]\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(errors, bins=30, kde=True)\n",
    "plt.title(\"Error distribution (y_true - y_pred)\")\n",
    "plt.xlabel(\"Error\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / f\"{MODEL_NAME}_plot_error_dist.png\", dpi=150)\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFM (venv)",
   "language": "python",
   "name": "tfm-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 41.3843,
   "end_time": "2026-02-02T11:43:10.860852",
   "environment_variables": {},
   "exception": null,
   "input_path": "/home/sagemaker-user/TFMAXEL/notebooks/07_Transformer_global_exog.ipynb",
   "output_path": "/home/sagemaker-user/TFMAXEL/outputs/notebook_runs/07_Transformer_global_exog.ipynb",
   "parameters": {},
   "start_time": "2026-02-02T11:42:29.476552",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}